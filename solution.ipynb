{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-87c762a3288b>:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  from tqdm._tqdm_notebook import tqdm_notebook\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import pandas as pd\n",
    "#import dask.dataframe as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from warnings import filterwarnings\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#from keras import Sequential\n",
    "#from keras.layers import Dense\n",
    "m = \"test\"\n",
    "\n",
    "%matplotlib inline\n",
    "filterwarnings('ignore')\n",
    "\n",
    "tqdm_notebook.pandas(desc=\"Progress:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считываем данные\n",
    "tr_mcc_codes = pd.read_csv('../data/tr_mcc_codes.csv', sep=';', index_col='mcc_code')\n",
    "tr_types = pd.read_csv('../data/tr_types.csv', sep=';', index_col='tr_type')\n",
    "\n",
    "transactions = pd.read_csv('../data/transactions.csv', index_col='customer_id')\n",
    "gender_train = pd.read_csv('../data/gender_train.csv', index_col='customer_id')\n",
    "gender_test = pd.read_csv('../data/gender_test.csv', index_col='customer_id')\n",
    "transactions_train = transactions.join(gender_train, how='inner')\n",
    "transactions_test = transactions.join(gender_test, how='inner')\n",
    "\n",
    "del transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции, которыми можно пользоваться для построения классификатора, \n",
    "# оценки его результатов и построение прогноза для тестовой части пользователей\n",
    "\n",
    "# Cross-validation score (среднее значение метрики ROC AUC на тренировочных данных)\n",
    "def cv_score(params, train, y_true):\n",
    "    cv_res=xgb.cv(params, xgb.DMatrix(train, y_true, silent = True),\n",
    "                  early_stopping_rounds=10, maximize=True, \n",
    "                  num_boost_round=(10000), nfold=5, stratified=True)\n",
    "    index_argmax = cv_res['test-auc-mean'].argmax()\n",
    "    #print('Cross-validation, ROC AUC: {:.3f}+-{:.3f}, Trees: {}'.format(cv_res.loc[index_argmax]['test-auc-mean'],\n",
    "    #                                                                    cv_res.loc[index_argmax]['test-auc-std'],\n",
    "    #                                                                    index_argmax))\n",
    "    return round(cv_res.loc[index_argmax]['test-auc-mean'], 4)\n",
    "\n",
    "# Построение модели + возврат результатов классификации тестовых пользователей\n",
    "def fit_predict(params, num_trees, train, test, target):\n",
    "    params['learning_rate'] = params['eta']\n",
    "    clf = xgb.train(params, xgb.DMatrix(train.values, target, feature_names=list(train.columns)), \n",
    "                    num_boost_round=num_trees, maximize=True)\n",
    "    y_pred = clf.predict(xgb.DMatrix(test.values, feature_names=list(train.columns)))\n",
    "    submission = pd.DataFrame(index=test.index, data=y_pred, columns=['probability'])\n",
    "    return clf, submission\n",
    "\n",
    "# Отрисовка важности переменных. Важность переменной - количество разбиений выборки, \n",
    "# в которых участвует данная переменная. Чем больше - тем она, вероятно, лучше \n",
    "def draw_feature_importances(clf, top_k=10):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    importances = dict(sorted(clf.get_score().items(), key=lambda x: x[1])[-top_k:])\n",
    "    y_pos = np.arange(len(importances))\n",
    "    \n",
    "    plt.barh(y_pos, list(importances.values()), align='center', color='green')\n",
    "    plt.yticks(y_pos, importances.keys(), fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.xlabel('Feature importance', fontsize=15)\n",
    "    plt.title('Features importances, Sberbank Gender Prediction', fontsize=18)\n",
    "    plt.ylim(-0.5, len(importances) - 0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_test[\"ind_day\"] = transactions_test.tr_datetime.apply(lambda x: int(x.split()[0]))\n",
    "transactions_train[\"ind_day\"] = transactions_train.tr_datetime.apply(lambda x: int(x.split()[0]))\n",
    "transactions_test[\"real_amount\"] = transactions_test.amount.apply(lambda x: int(x) / (np.pi ** np.e))\n",
    "transactions_train[\"real_amount\"] = transactions_train.amount.apply(lambda x: int(x) / (np.pi ** np.e))\n",
    "transactions_test[\"real_date\"] = transactions_test.ind_day.apply(lambda x: dt(2018,1,1) + td(days=(int(x)-153)))\n",
    "transactions_train[\"real_date\"] = transactions_train.ind_day.apply(lambda x: dt(2018,1,1) + td(days=(int(x)-153)))\n",
    "transactions_test[\"hour\"] = transactions_test.tr_datetime.apply(lambda x: int(x.split()[1].split(\":\")[0]))\n",
    "transactions_train[\"hour\"] = transactions_train.tr_datetime.apply(lambda x: int(x.split()[1].split(\":\")[0]))\n",
    "\n",
    "for df in [transactions_train, transactions_test]:\n",
    "    df['day'] = df['tr_datetime'].str.split().apply(lambda x: int(x[0]) % 7)\n",
    "    df['hour'] = df['tr_datetime'].apply(lambda x: re.search(' \\d*', x).group(0)).astype(int)\n",
    "    df['night'] = ~df['hour'].between(6, 22).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f00;color:#fff;padding:10px;text-align:center\"><h2>Создание фичей</h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543b96bea8ed4903b8481ca5f49b071f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Progress:', max=8400.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9102862566647e2853c8dd5cce866c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Progress:', max=3600.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def my_features(x): \n",
    "    features = []\n",
    "\n",
    "    features.append(pd.Series(data=x[(x['amount'] < 0)]['amount'].agg(['mean'])\\\n",
    "                              .add_prefix('amount_')))       \n",
    "    \n",
    "    return pd.concat(features)\n",
    "\n",
    "data_train = transactions_train.groupby(transactions_train.index)\\\n",
    "                               .progress_apply(my_features).unstack(-1)\n",
    "data_test = transactions_test.groupby(transactions_test.index)\\\n",
    "                             .progress_apply(my_features).unstack(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.unstack().T\n",
    "data_test = data_test.unstack().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#fc0\">Праздники</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfae45e121b24dd192ce1749561bd809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Progress:', max=8400.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955b95691a92470a931f89abe9f66507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Progress:', max=3600.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def my_features(x): \n",
    "    features = []\n",
    "\n",
    "    features.append(pd.Series(data=x[(x['amount']<0) & (x['mcc_code']==5992) & \n",
    "                              (x['real_date'] <= dt(2018,3,8)) & (x['real_date'] >= dt(2018,3,6))]['amount']\\\n",
    "                              .agg(['sum']).add_prefix('5992_')))  \n",
    "\n",
    "    features.append(pd.Series(data=x[(x['amount']<0) & (x['mcc_code']==5944) & \n",
    "                              (x['real_date'] <= dt(2018,3,8)) & (x['real_date'] >= dt(2018,3,6))]['amount']\\\n",
    "                              .agg(['sum']).add_prefix('5944_'))) \n",
    "\n",
    "    features.append(pd.Series(data=x[(x['amount']<0) & (x['mcc_code']==5611) & \n",
    "                              (x['real_date'] <= dt(2018,2,23)) & (x['real_date'] >= dt(2018,2,18))]['amount']\\\n",
    "                              .agg(['sum']).add_prefix('5611_')))  \n",
    "\n",
    "    features.append(pd.Series(data=x[(x['amount']<0) & (x['mcc_code']==5947) & \n",
    "                              (x['real_date'] <= dt(2018,2,23)) & (x['real_date'] >= dt(2018,2,18))]['amount']\\\n",
    "                              .agg(['sum']).add_prefix('5947_')))      \n",
    "    \n",
    "    return pd.concat(features)\n",
    "\n",
    "\n",
    "new_train = transactions_train.groupby(transactions_train.index)\\\n",
    "                               .progress_apply(my_features).unstack(-1)\n",
    "new_test = transactions_test.groupby(transactions_test.index)\\\n",
    "                             .progress_apply(my_features).unstack(-1)\n",
    "data_train = pd.merge(data_train, new_train.unstack().T, left_index=True, right_index=True)\n",
    "data_test = pd.merge(data_test, new_test.unstack().T, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#fc0\">TOP20 tr_type</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea20923426740cc8dbbe6198dd44ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Progress:', max=8400.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bc2427d5df4628971d451fa6b6f43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Progress:', max=3600.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def my_features(x): \n",
    "    features = []\n",
    "    \n",
    "    tr_types = [6210, 8100, 7075, 1310, 7034, 1410, 6200, 7014, 7074, 6010, 4041,\n",
    "            2460, 7040, 4210, 2320, 4020, 6100, 7030, 2440, 7071]\n",
    "    features.append(pd.Series(data=map(lambda y: x[x.tr_type == y]['amount'].count(), tr_types),\n",
    "                              index=tr_types).add_prefix('tr_type_20_'))\n",
    "    \n",
    "    return pd.concat(features)\n",
    "\n",
    "new_train = transactions_train.groupby(transactions_train.index)\\\n",
    "                               .progress_apply(my_features).unstack(-1)\n",
    "new_test = transactions_test.groupby(transactions_test.index)\\\n",
    "                             .progress_apply(my_features).unstack(-1)\n",
    "data_train = pd.merge(data_train, new_train.unstack().T, left_index=True, right_index=True)\n",
    "data_test = pd.merge(data_test, new_test.unstack().T, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#fc0\">TOP mcc_code</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47efc0a7bd604561af2f6b9fcdfd3e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Progress:', max=8400.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22898d45f87e467f97e25024f88cfc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Progress:', max=3600.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def my_features(x): \n",
    "    features = []\n",
    "    \n",
    "    mcc_codes = [5967, 5965, 6211, 5931, 7995, 7994, 4784, 7512, 7993, 5940, 3501,\n",
    "            5621, 5949, 5532, 5013, 5533, 5631, 7538, 5714, 5977, 6051, 5511,\n",
    "             742, 7542, 5733, 7372, 4214, 7933, 5816, 5541, 5045, 5651, 5065,\n",
    "            7011, 5734, 5699, 5462, 7278, 5661, 8398, 5691, 5964]\n",
    "    features.append(pd.Series(data=map(lambda y: x[x.mcc_code == y]['amount'].count(), mcc_codes),\n",
    "                              index=mcc_codes).add_prefix('mcc_code_'))\n",
    "    \n",
    "    return pd.concat(features)\n",
    "\n",
    "new_train = transactions_train.groupby(transactions_train.index)\\\n",
    "                               .progress_apply(my_features).unstack(-1)\n",
    "new_test = transactions_test.groupby(transactions_test.index)\\\n",
    "                             .progress_apply(my_features).unstack(-1)\n",
    "data_train = pd.merge(data_train, new_train.unstack().T, left_index=True, right_index=True)\n",
    "data_test = pd.merge(data_test, new_test.unstack().T, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#fc0\">Default</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f405764d4525449fbee2188c70934bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Progress:', max=8400.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99bc118aa1d4557b714b00930688be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Progress:', max=3600.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def my_features(x): \n",
    "    features = []\n",
    "    \n",
    "    features.append(pd.Series(x['day'].value_counts(normalize=True).add_prefix('day_')))\n",
    "    features.append(pd.Series(x['hour'].value_counts(normalize=True).add_prefix('hour_')))\n",
    "    features.append(pd.Series(x['night'].value_counts(normalize=True).add_prefix('night_')))\n",
    "    features.append(pd.Series(x[x['amount']>0]['amount'].agg(['min', 'max', 'mean', 'median', 'std', 'count'])\\\n",
    "                                                        .add_prefix('positive_transactions_')))\n",
    "    features.append(pd.Series(x[x['amount']<0]['amount'].agg(['min', 'max', 'mean', 'median', 'std', 'count'])\\\n",
    "                                                        .add_prefix('negative_transactions_')))\n",
    "    \n",
    "    return pd.concat(features)\n",
    "\n",
    "new_train = transactions_train.groupby(transactions_train.index)\\\n",
    "                               .progress_apply(my_features).unstack(-1)\n",
    "new_test = transactions_test.groupby(transactions_test.index)\\\n",
    "                             .progress_apply(my_features).unstack(-1)\n",
    "\n",
    "data_train = pd.merge(data_train, new_train, left_index=True, right_index=True)\n",
    "data_test = pd.merge(data_test, new_test, left_index=True, right_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#fc0\">mcc codes stat</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0919d4eedcaa489086bf962e4c6149fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Progress:', max=8400.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd047ef5214401396fb5bbb00a971f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Progress:', max=3600.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def my_features(x): \n",
    "    features = []\n",
    "    \n",
    "    mcc = {\n",
    "        'min': [4814, 6010, 5977, 5812, 5200, 5945, 4900, 5211, 5411, 5814, 5735, 5541, 5311, 5641, 4829, 5499],\n",
    "        'std': [5814, 5311, 5735, 5541, 5641, 4829, 5211],\n",
    "        'count': [5977, 5691, 5661, 4112, 5912, 4814, 6010, 5541, 5211, 5921],\n",
    "        'sum': [5541, 5921, 5812, 5735, 6011, 6010, 4829, 5814, 5977, 5661, 5912],\n",
    "        'median': [8999, 4829, 4112, 5211],\n",
    "     }\n",
    "    \n",
    "    for i in mcc:\n",
    "        for n in mcc[i]:\n",
    "            features.append(pd.Series(data=x[(x['amount']<0) & (x['mcc_code']==n)]['amount'].agg([i])\\\n",
    "                                      .add_prefix(str(n)+'_')))\n",
    "    \n",
    "    return pd.concat(features)\n",
    "\n",
    "new_train = transactions_train.groupby(transactions_train.index)\\\n",
    "                               .progress_apply(my_features).unstack(-1)\n",
    "new_test = transactions_test.groupby(transactions_test.index)\\\n",
    "                             .progress_apply(my_features).unstack(-1)\n",
    "data_train = pd.merge(data_train, new_train.unstack().T, left_index=True, right_index=True)\n",
    "data_test = pd.merge(data_test, new_test.unstack().T, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f00;color:#fff;padding:10px;text-align:center\"><h2>Подготовка данных</h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_g = data_train.join(gender_train, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(data_train_g.columns)\n",
    "\n",
    "#features = features[5:]\n",
    "\n",
    "my_data_train_g = data_train_g[features[:]]\n",
    "my_data_train = data_train[features[:-1]]\n",
    "my_data_test = data_test[features[:-1]]\n",
    "\n",
    "#for i in features[:-1]:\n",
    "#    my_data_train_g[i] = my_data_train_g[i].map(lambda x: 1 if x > 0 else 0)\n",
    "#    my_data_train[i] = my_data_train[i].map(lambda x: 1 if x > 0 else 0)\n",
    "#    my_data_test[i] = my_data_test[i].map(lambda x: 1 if x > 0 else 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_train_g.to_csv(\"my_data_train_g.csv\")\n",
    "my_data_train.to_csv(\"my_data_train.csv\")\n",
    "my_data_test.to_csv(\"my_data_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_train_g = pd.read_csv(\"my_data_train_g.csv\")\n",
    "my_data_train = pd.read_csv(\"my_data_train.csv\")\n",
    "my_data_test = pd.read_csv(\"my_data_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = list(my_data_train_g.iloc[:, len(features)-1])\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(my_data_train.fillna(0))\n",
    "\n",
    "\n",
    "#X, X_test, Y, y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f00;color:#fff;padding:10px;text-align:center\"><h2>XGBoost</h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"target = my_data_train_g['gender']\n",
    "\n",
    "m = 0\n",
    "best_param = {}\n",
    "\n",
    "for min_child_weight in [0, 1, 3, 5, 7, 10]:\n",
    "    for gamma in [0, 0.1, 0.5, 1, 1.5, 2, 5]:\n",
    "        for subsample in [0.5, 0.6, 0.8, 1.0]:\n",
    "            for colsample_bytree in [0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "                for max_depth in [3, 4, 6, 10]:\n",
    "                    for eta in [0.01, 0.05, 0.1, 0.2]:\n",
    "                        for lambda_ in [0.5, 0.7, 1.0]:\n",
    "                            for alpha in [0.0, 0.1, 0.2, 0.5, 0.7]:\n",
    "\n",
    "                                params_xb = {\n",
    "                                    'eta': eta,\n",
    "                                    'max_depth': max_depth, #3,\n",
    "                                    'subsample': subsample, #0.8,\n",
    "                                    'colsample_bytree': colsample_bytree, #0.8,\n",
    "\n",
    "                                    'gamma': gamma, #0,\n",
    "                                    'lambda': lambda_,\n",
    "                                    'alpha': alpha,\n",
    "                                    'min_child_weight': min_child_weight, #0,\n",
    "\n",
    "                                    'eval_metric': 'auc',\n",
    "                                    'objective': 'binary:logistic' ,\n",
    "                                    'booster': 'gbtree',\n",
    "                                    #'njobs': -1,\n",
    "                                    #'tree_method': 'approx',\n",
    "                                    'tree_method': 'gpu_hist',\n",
    "                                    'gpu_id': 0\n",
    "                                }\n",
    "                                sc = cv_score(params_xb, my_data_train, target)\n",
    "                                if m < sc:\n",
    "                                    m = sc\n",
    "                                    best_param = params_xb\n",
    "                                    print(m, min_child_weight, gamma, subsample, colsample_bytree, max_depth, eta, lambda_, alpha)\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf, submission = fit_predict(best_param, 70, my_data_train, my_data_test, target)\n",
    "#draw_feature_importances(clf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8606"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_xb = {\n",
    "    'eta': 0.3,\n",
    "    'max_depth': 4, #3,\n",
    "    'subsample': 0.8, #0.8,\n",
    "    'colsample_bytree': 0.9, #0.8,\n",
    "\n",
    "    'gamma': .2, #0,\n",
    "    'lambda': 0.5,\n",
    "    'alpha': .5,\n",
    "    'min_child_weight': 7, #0,\n",
    "\n",
    "    'eval_metric': 'auc',\n",
    "    'objective': 'binary:logistic' ,\n",
    "    'booster': 'gbtree',\n",
    "    #'njobs': -1,\n",
    "    #'tree_method': 'approx',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'gpu_id': 0\n",
    "}\n",
    "target = my_data_train_g['gender']\n",
    "sc = cv_score(params_xb, my_data_train, target)\n",
    "clf, submission = fit_predict(params_xb, 70, my_data_train, my_data_test, target)\n",
    "draw_feature_importances(clf, 10)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#fc0\">Запись результатов</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "submission.to_csv(f'../results/submission_xgboost_{m}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1136.2994556427002, 18.93832426071167)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time() - start_time, (time.time() - start_time)/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f00;color:#fff;padding:10px;text-align:center\"><h2>Keras</h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    {\n",
    "        \"act\": \"tanh\",\n",
    "        \"num\": 8\n",
    "    },\n",
    "    {\n",
    "        \"act\": \"tanh\",\n",
    "        \"num\": 8\n",
    "    },\n",
    "    {\n",
    "        \"act\": \"sigmoid\",\n",
    "        \"epoch\": 75,\n",
    "        'batch': 10\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "8400/8400 [==============================] - ETA: 1:18 - loss: 0.6927 - accuracy: 0.60 - ETA: 2s - loss: 0.6905 - accuracy: 0.6000 - ETA: 1s - loss: 0.6760 - accuracy: 0.65 - ETA: 1s - loss: 0.6578 - accuracy: 0.66 - ETA: 0s - loss: 0.6350 - accuracy: 0.68 - ETA: 0s - loss: 0.6152 - accuracy: 0.69 - ETA: 0s - loss: 0.6037 - accuracy: 0.69 - ETA: 0s - loss: 0.5925 - accuracy: 0.70 - ETA: 0s - loss: 0.5802 - accuracy: 0.71 - ETA: 0s - loss: 0.5759 - accuracy: 0.71 - ETA: 0s - loss: 0.5677 - accuracy: 0.71 - ETA: 0s - loss: 0.5598 - accuracy: 0.72 - ETA: 0s - loss: 0.5548 - accuracy: 0.72 - ETA: 0s - loss: 0.5498 - accuracy: 0.72 - ETA: 0s - loss: 0.5481 - accuracy: 0.72 - ETA: 0s - loss: 0.5448 - accuracy: 0.72 - 1s 103us/step - loss: 0.5443 - accuracy: 0.7296\n",
      "Epoch 2/75\n",
      "8400/8400 [==============================] - ETA: 1:20 - loss: 0.3927 - accuracy: 0.80 - ETA: 2s - loss: 0.5014 - accuracy: 0.7509 - ETA: 1s - loss: 0.4978 - accuracy: 0.75 - ETA: 0s - loss: 0.4853 - accuracy: 0.76 - ETA: 0s - loss: 0.4951 - accuracy: 0.75 - ETA: 0s - loss: 0.4992 - accuracy: 0.75 - ETA: 0s - loss: 0.4936 - accuracy: 0.76 - ETA: 0s - loss: 0.4949 - accuracy: 0.76 - ETA: 0s - loss: 0.4988 - accuracy: 0.76 - ETA: 0s - loss: 0.4975 - accuracy: 0.76 - ETA: 0s - loss: 0.4992 - accuracy: 0.75 - ETA: 0s - loss: 0.4980 - accuracy: 0.75 - ETA: 0s - loss: 0.4953 - accuracy: 0.76 - ETA: 0s - loss: 0.4958 - accuracy: 0.76 - 1s 96us/step - loss: 0.4958 - accuracy: 0.7606\n",
      "Epoch 3/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.7044 - accuracy: 0.40 - ETA: 0s - loss: 0.4572 - accuracy: 0.79 - ETA: 0s - loss: 0.4988 - accuracy: 0.75 - ETA: 0s - loss: 0.4890 - accuracy: 0.76 - ETA: 0s - loss: 0.4876 - accuracy: 0.76 - ETA: 0s - loss: 0.4836 - accuracy: 0.76 - ETA: 0s - loss: 0.4848 - accuracy: 0.76 - ETA: 0s - loss: 0.4853 - accuracy: 0.76 - ETA: 0s - loss: 0.4839 - accuracy: 0.77 - ETA: 0s - loss: 0.4845 - accuracy: 0.77 - ETA: 0s - loss: 0.4876 - accuracy: 0.76 - ETA: 0s - loss: 0.4909 - accuracy: 0.76 - ETA: 0s - loss: 0.4890 - accuracy: 0.76 - ETA: 0s - loss: 0.4900 - accuracy: 0.76 - 1s 80us/step - loss: 0.4896 - accuracy: 0.7676\n",
      "Epoch 4/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.3043 - accuracy: 1.00 - ETA: 0s - loss: 0.4798 - accuracy: 0.77 - ETA: 0s - loss: 0.4663 - accuracy: 0.77 - ETA: 0s - loss: 0.4762 - accuracy: 0.77 - ETA: 0s - loss: 0.4766 - accuracy: 0.77 - ETA: 0s - loss: 0.4835 - accuracy: 0.76 - ETA: 0s - loss: 0.4878 - accuracy: 0.76 - ETA: 0s - loss: 0.4851 - accuracy: 0.76 - ETA: 0s - loss: 0.4873 - accuracy: 0.76 - ETA: 0s - loss: 0.4859 - accuracy: 0.76 - ETA: 0s - loss: 0.4871 - accuracy: 0.76 - ETA: 0s - loss: 0.4856 - accuracy: 0.76 - ETA: 0s - loss: 0.4825 - accuracy: 0.76 - ETA: 0s - loss: 0.4805 - accuracy: 0.76 - 1s 85us/step - loss: 0.4842 - accuracy: 0.7669\n",
      "Epoch 5/75\n",
      "8400/8400 [==============================] - ETA: 0s - loss: 0.5154 - accuracy: 0.80 - ETA: 0s - loss: 0.4667 - accuracy: 0.76 - ETA: 0s - loss: 0.4846 - accuracy: 0.75 - ETA: 0s - loss: 0.4786 - accuracy: 0.76 - ETA: 0s - loss: 0.4772 - accuracy: 0.76 - ETA: 0s - loss: 0.4741 - accuracy: 0.77 - ETA: 0s - loss: 0.4839 - accuracy: 0.76 - ETA: 0s - loss: 0.4834 - accuracy: 0.76 - ETA: 0s - loss: 0.4861 - accuracy: 0.76 - ETA: 0s - loss: 0.4851 - accuracy: 0.76 - ETA: 0s - loss: 0.4843 - accuracy: 0.76 - ETA: 0s - loss: 0.4838 - accuracy: 0.76 - ETA: 0s - loss: 0.4808 - accuracy: 0.76 - ETA: 0s - loss: 0.4790 - accuracy: 0.76 - 1s 83us/step - loss: 0.4805 - accuracy: 0.7662\n",
      "Epoch 6/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.5940 - accuracy: 0.80 - ETA: 0s - loss: 0.4534 - accuracy: 0.78 - ETA: 0s - loss: 0.4682 - accuracy: 0.77 - ETA: 0s - loss: 0.4601 - accuracy: 0.77 - ETA: 0s - loss: 0.4613 - accuracy: 0.77 - ETA: 0s - loss: 0.4673 - accuracy: 0.77 - ETA: 0s - loss: 0.4646 - accuracy: 0.77 - ETA: 0s - loss: 0.4713 - accuracy: 0.77 - ETA: 0s - loss: 0.4749 - accuracy: 0.77 - ETA: 0s - loss: 0.4791 - accuracy: 0.76 - ETA: 0s - loss: 0.4788 - accuracy: 0.77 - ETA: 0s - loss: 0.4765 - accuracy: 0.77 - ETA: 0s - loss: 0.4743 - accuracy: 0.77 - ETA: 0s - loss: 0.4749 - accuracy: 0.77 - ETA: 0s - loss: 0.4753 - accuracy: 0.77 - 1s 89us/step - loss: 0.4758 - accuracy: 0.7736\n",
      "Epoch 7/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.5694 - accuracy: 0.80 - ETA: 0s - loss: 0.4636 - accuracy: 0.78 - ETA: 0s - loss: 0.4589 - accuracy: 0.79 - ETA: 0s - loss: 0.4656 - accuracy: 0.78 - ETA: 0s - loss: 0.4655 - accuracy: 0.78 - ETA: 0s - loss: 0.4648 - accuracy: 0.78 - ETA: 0s - loss: 0.4717 - accuracy: 0.77 - ETA: 0s - loss: 0.4675 - accuracy: 0.77 - ETA: 0s - loss: 0.4700 - accuracy: 0.77 - ETA: 0s - loss: 0.4716 - accuracy: 0.77 - ETA: 0s - loss: 0.4731 - accuracy: 0.77 - ETA: 0s - loss: 0.4723 - accuracy: 0.77 - ETA: 0s - loss: 0.4739 - accuracy: 0.77 - ETA: 0s - loss: 0.4735 - accuracy: 0.77 - ETA: 0s - loss: 0.4726 - accuracy: 0.77 - 1s 88us/step - loss: 0.4730 - accuracy: 0.7731\n",
      "Epoch 8/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.5645 - accuracy: 0.50 - ETA: 0s - loss: 0.4845 - accuracy: 0.74 - ETA: 0s - loss: 0.4616 - accuracy: 0.77 - ETA: 0s - loss: 0.4610 - accuracy: 0.77 - ETA: 0s - loss: 0.4690 - accuracy: 0.77 - ETA: 0s - loss: 0.4656 - accuracy: 0.77 - ETA: 0s - loss: 0.4667 - accuracy: 0.77 - ETA: 0s - loss: 0.4643 - accuracy: 0.77 - ETA: 0s - loss: 0.4630 - accuracy: 0.77 - ETA: 0s - loss: 0.4680 - accuracy: 0.77 - ETA: 0s - loss: 0.4685 - accuracy: 0.77 - ETA: 0s - loss: 0.4693 - accuracy: 0.77 - ETA: 0s - loss: 0.4695 - accuracy: 0.77 - ETA: 0s - loss: 0.4687 - accuracy: 0.77 - 1s 84us/step - loss: 0.4700 - accuracy: 0.7767\n",
      "Epoch 9/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2481 - accuracy: 1.00 - ETA: 0s - loss: 0.4431 - accuracy: 0.80 - ETA: 0s - loss: 0.4618 - accuracy: 0.78 - ETA: 0s - loss: 0.4613 - accuracy: 0.78 - ETA: 0s - loss: 0.4615 - accuracy: 0.78 - ETA: 0s - loss: 0.4644 - accuracy: 0.78 - ETA: 0s - loss: 0.4652 - accuracy: 0.77 - ETA: 0s - loss: 0.4638 - accuracy: 0.77 - ETA: 0s - loss: 0.4630 - accuracy: 0.77 - ETA: 0s - loss: 0.4643 - accuracy: 0.77 - ETA: 0s - loss: 0.4637 - accuracy: 0.77 - ETA: 0s - loss: 0.4652 - accuracy: 0.77 - ETA: 0s - loss: 0.4676 - accuracy: 0.77 - ETA: 0s - loss: 0.4679 - accuracy: 0.77 - 1s 82us/step - loss: 0.4669 - accuracy: 0.7773\n",
      "Epoch 10/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2969 - accuracy: 1.00 - ETA: 0s - loss: 0.4608 - accuracy: 0.78 - ETA: 0s - loss: 0.4649 - accuracy: 0.77 - ETA: 0s - loss: 0.4573 - accuracy: 0.78 - ETA: 0s - loss: 0.4671 - accuracy: 0.77 - ETA: 0s - loss: 0.4625 - accuracy: 0.77 - ETA: 0s - loss: 0.4579 - accuracy: 0.77 - ETA: 0s - loss: 0.4587 - accuracy: 0.77 - ETA: 0s - loss: 0.4598 - accuracy: 0.77 - ETA: 0s - loss: 0.4612 - accuracy: 0.77 - ETA: 0s - loss: 0.4643 - accuracy: 0.77 - ETA: 0s - loss: 0.4646 - accuracy: 0.77 - ETA: 0s - loss: 0.4635 - accuracy: 0.77 - ETA: 0s - loss: 0.4625 - accuracy: 0.77 - 1s 83us/step - loss: 0.4635 - accuracy: 0.7763\n",
      "Epoch 11/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2060 - accuracy: 1.00 - ETA: 0s - loss: 0.4370 - accuracy: 0.79 - ETA: 0s - loss: 0.4468 - accuracy: 0.78 - ETA: 0s - loss: 0.4515 - accuracy: 0.78 - ETA: 0s - loss: 0.4508 - accuracy: 0.79 - ETA: 0s - loss: 0.4603 - accuracy: 0.77 - ETA: 0s - loss: 0.4544 - accuracy: 0.78 - ETA: 0s - loss: 0.4557 - accuracy: 0.78 - ETA: 0s - loss: 0.4563 - accuracy: 0.78 - ETA: 0s - loss: 0.4598 - accuracy: 0.77 - ETA: 0s - loss: 0.4635 - accuracy: 0.77 - ETA: 0s - loss: 0.4612 - accuracy: 0.77 - ETA: 0s - loss: 0.4624 - accuracy: 0.77 - ETA: 0s - loss: 0.4618 - accuracy: 0.77 - 1s 81us/step - loss: 0.4610 - accuracy: 0.7804\n",
      "Epoch 12/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.3839 - accuracy: 0.80 - ETA: 0s - loss: 0.4527 - accuracy: 0.78 - ETA: 0s - loss: 0.4552 - accuracy: 0.78 - ETA: 0s - loss: 0.4585 - accuracy: 0.78 - ETA: 0s - loss: 0.4627 - accuracy: 0.77 - ETA: 0s - loss: 0.4591 - accuracy: 0.77 - ETA: 0s - loss: 0.4578 - accuracy: 0.78 - ETA: 0s - loss: 0.4632 - accuracy: 0.77 - ETA: 0s - loss: 0.4647 - accuracy: 0.78 - ETA: 0s - loss: 0.4657 - accuracy: 0.77 - ETA: 0s - loss: 0.4647 - accuracy: 0.77 - ETA: 0s - loss: 0.4641 - accuracy: 0.77 - ETA: 0s - loss: 0.4597 - accuracy: 0.78 - ETA: 0s - loss: 0.4582 - accuracy: 0.77 - 1s 82us/step - loss: 0.4590 - accuracy: 0.7790\n",
      "Epoch 13/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.1201 - accuracy: 1.00 - ETA: 0s - loss: 0.4206 - accuracy: 0.80 - ETA: 0s - loss: 0.4263 - accuracy: 0.80 - ETA: 0s - loss: 0.4315 - accuracy: 0.80 - ETA: 0s - loss: 0.4383 - accuracy: 0.79 - ETA: 0s - loss: 0.4438 - accuracy: 0.79 - ETA: 0s - loss: 0.4429 - accuracy: 0.79 - ETA: 0s - loss: 0.4452 - accuracy: 0.79 - ETA: 0s - loss: 0.4487 - accuracy: 0.78 - ETA: 0s - loss: 0.4523 - accuracy: 0.78 - ETA: 0s - loss: 0.4533 - accuracy: 0.78 - ETA: 0s - loss: 0.4553 - accuracy: 0.78 - ETA: 0s - loss: 0.4537 - accuracy: 0.78 - ETA: 0s - loss: 0.4543 - accuracy: 0.78 - 1s 84us/step - loss: 0.4562 - accuracy: 0.7820\n",
      "Epoch 14/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.3221 - accuracy: 0.90 - ETA: 0s - loss: 0.4467 - accuracy: 0.79 - ETA: 0s - loss: 0.4456 - accuracy: 0.79 - ETA: 0s - loss: 0.4432 - accuracy: 0.79 - ETA: 0s - loss: 0.4424 - accuracy: 0.80 - ETA: 0s - loss: 0.4503 - accuracy: 0.79 - ETA: 0s - loss: 0.4542 - accuracy: 0.79 - ETA: 0s - loss: 0.4591 - accuracy: 0.78 - ETA: 0s - loss: 0.4566 - accuracy: 0.78 - ETA: 0s - loss: 0.4588 - accuracy: 0.78 - ETA: 0s - loss: 0.4608 - accuracy: 0.78 - ETA: 0s - loss: 0.4575 - accuracy: 0.78 - ETA: 0s - loss: 0.4531 - accuracy: 0.78 - ETA: 0s - loss: 0.4525 - accuracy: 0.78 - 1s 84us/step - loss: 0.4533 - accuracy: 0.7837\n",
      "Epoch 15/75\n",
      "8400/8400 [==============================] - ETA: 0s - loss: 0.2101 - accuracy: 1.00 - ETA: 0s - loss: 0.4404 - accuracy: 0.80 - ETA: 0s - loss: 0.4153 - accuracy: 0.81 - ETA: 0s - loss: 0.4188 - accuracy: 0.80 - ETA: 0s - loss: 0.4374 - accuracy: 0.79 - ETA: 0s - loss: 0.4365 - accuracy: 0.79 - ETA: 0s - loss: 0.4405 - accuracy: 0.79 - ETA: 0s - loss: 0.4473 - accuracy: 0.79 - ETA: 0s - loss: 0.4467 - accuracy: 0.79 - ETA: 0s - loss: 0.4483 - accuracy: 0.79 - ETA: 0s - loss: 0.4491 - accuracy: 0.79 - ETA: 0s - loss: 0.4503 - accuracy: 0.78 - ETA: 0s - loss: 0.4497 - accuracy: 0.78 - ETA: 0s - loss: 0.4517 - accuracy: 0.78 - 1s 84us/step - loss: 0.4520 - accuracy: 0.7871\n",
      "Epoch 16/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.5356 - accuracy: 0.80 - ETA: 0s - loss: 0.4323 - accuracy: 0.80 - ETA: 0s - loss: 0.4409 - accuracy: 0.79 - ETA: 0s - loss: 0.4525 - accuracy: 0.78 - ETA: 0s - loss: 0.4498 - accuracy: 0.79 - ETA: 0s - loss: 0.4578 - accuracy: 0.78 - ETA: 0s - loss: 0.4555 - accuracy: 0.78 - ETA: 0s - loss: 0.4516 - accuracy: 0.78 - ETA: 0s - loss: 0.4524 - accuracy: 0.78 - ETA: 0s - loss: 0.4528 - accuracy: 0.78 - ETA: 0s - loss: 0.4506 - accuracy: 0.79 - ETA: 0s - loss: 0.4488 - accuracy: 0.79 - ETA: 0s - loss: 0.4492 - accuracy: 0.79 - ETA: 0s - loss: 0.4506 - accuracy: 0.79 - 1s 84us/step - loss: 0.4486 - accuracy: 0.7912\n",
      "Epoch 17/75\n",
      "8400/8400 [==============================] - ETA: 0s - loss: 0.3108 - accuracy: 0.80 - ETA: 0s - loss: 0.4580 - accuracy: 0.78 - ETA: 0s - loss: 0.4587 - accuracy: 0.78 - ETA: 0s - loss: 0.4490 - accuracy: 0.79 - ETA: 0s - loss: 0.4421 - accuracy: 0.79 - ETA: 0s - loss: 0.4455 - accuracy: 0.78 - ETA: 0s - loss: 0.4499 - accuracy: 0.78 - ETA: 0s - loss: 0.4470 - accuracy: 0.78 - ETA: 0s - loss: 0.4472 - accuracy: 0.79 - ETA: 0s - loss: 0.4454 - accuracy: 0.79 - ETA: 0s - loss: 0.4473 - accuracy: 0.79 - ETA: 0s - loss: 0.4472 - accuracy: 0.79 - ETA: 0s - loss: 0.4507 - accuracy: 0.78 - ETA: 0s - loss: 0.4481 - accuracy: 0.79 - 1s 84us/step - loss: 0.4472 - accuracy: 0.7910\n",
      "Epoch 18/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.5114 - accuracy: 0.60 - ETA: 0s - loss: 0.4471 - accuracy: 0.77 - ETA: 0s - loss: 0.4472 - accuracy: 0.77 - ETA: 0s - loss: 0.4378 - accuracy: 0.78 - ETA: 0s - loss: 0.4269 - accuracy: 0.79 - ETA: 0s - loss: 0.4352 - accuracy: 0.79 - ETA: 0s - loss: 0.4439 - accuracy: 0.79 - ETA: 0s - loss: 0.4443 - accuracy: 0.78 - ETA: 0s - loss: 0.4444 - accuracy: 0.79 - ETA: 0s - loss: 0.4450 - accuracy: 0.79 - ETA: 0s - loss: 0.4450 - accuracy: 0.78 - ETA: 0s - loss: 0.4442 - accuracy: 0.79 - ETA: 0s - loss: 0.4457 - accuracy: 0.79 - ETA: 0s - loss: 0.4443 - accuracy: 0.79 - 1s 84us/step - loss: 0.4459 - accuracy: 0.7926\n",
      "Epoch 19/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.6315 - accuracy: 0.70 - ETA: 0s - loss: 0.4722 - accuracy: 0.78 - ETA: 0s - loss: 0.4739 - accuracy: 0.78 - ETA: 0s - loss: 0.4625 - accuracy: 0.78 - ETA: 0s - loss: 0.4537 - accuracy: 0.78 - ETA: 0s - loss: 0.4444 - accuracy: 0.79 - ETA: 0s - loss: 0.4411 - accuracy: 0.79 - ETA: 0s - loss: 0.4415 - accuracy: 0.79 - ETA: 0s - loss: 0.4403 - accuracy: 0.79 - ETA: 0s - loss: 0.4402 - accuracy: 0.79 - ETA: 0s - loss: 0.4402 - accuracy: 0.79 - ETA: 0s - loss: 0.4422 - accuracy: 0.79 - ETA: 0s - loss: 0.4449 - accuracy: 0.79 - ETA: 0s - loss: 0.4454 - accuracy: 0.79 - 1s 83us/step - loss: 0.4433 - accuracy: 0.7945\n",
      "Epoch 20/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.6865 - accuracy: 0.70 - ETA: 0s - loss: 0.3972 - accuracy: 0.82 - ETA: 0s - loss: 0.4157 - accuracy: 0.81 - ETA: 0s - loss: 0.4261 - accuracy: 0.80 - ETA: 0s - loss: 0.4356 - accuracy: 0.79 - ETA: 0s - loss: 0.4367 - accuracy: 0.79 - ETA: 0s - loss: 0.4338 - accuracy: 0.79 - ETA: 0s - loss: 0.4387 - accuracy: 0.79 - ETA: 0s - loss: 0.4420 - accuracy: 0.79 - ETA: 0s - loss: 0.4428 - accuracy: 0.79 - ETA: 0s - loss: 0.4403 - accuracy: 0.79 - ETA: 0s - loss: 0.4412 - accuracy: 0.79 - ETA: 0s - loss: 0.4434 - accuracy: 0.79 - ETA: 0s - loss: 0.4430 - accuracy: 0.79 - 1s 83us/step - loss: 0.4423 - accuracy: 0.7956\n",
      "Epoch 21/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.4359 - accuracy: 0.80 - ETA: 0s - loss: 0.4303 - accuracy: 0.79 - ETA: 0s - loss: 0.4411 - accuracy: 0.79 - ETA: 0s - loss: 0.4452 - accuracy: 0.78 - ETA: 0s - loss: 0.4475 - accuracy: 0.78 - ETA: 0s - loss: 0.4456 - accuracy: 0.79 - ETA: 0s - loss: 0.4429 - accuracy: 0.79 - ETA: 0s - loss: 0.4425 - accuracy: 0.79 - ETA: 0s - loss: 0.4440 - accuracy: 0.79 - ETA: 0s - loss: 0.4416 - accuracy: 0.79 - ETA: 0s - loss: 0.4414 - accuracy: 0.79 - ETA: 0s - loss: 0.4428 - accuracy: 0.79 - ETA: 0s - loss: 0.4434 - accuracy: 0.79 - ETA: 0s - loss: 0.4423 - accuracy: 0.79 - 1s 84us/step - loss: 0.4400 - accuracy: 0.7962\n",
      "Epoch 22/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2285 - accuracy: 0.90 - ETA: 0s - loss: 0.4281 - accuracy: 0.80 - ETA: 0s - loss: 0.4197 - accuracy: 0.81 - ETA: 0s - loss: 0.4300 - accuracy: 0.80 - ETA: 0s - loss: 0.4346 - accuracy: 0.79 - ETA: 0s - loss: 0.4428 - accuracy: 0.79 - ETA: 0s - loss: 0.4383 - accuracy: 0.79 - ETA: 0s - loss: 0.4366 - accuracy: 0.79 - ETA: 0s - loss: 0.4343 - accuracy: 0.79 - ETA: 0s - loss: 0.4370 - accuracy: 0.79 - ETA: 0s - loss: 0.4387 - accuracy: 0.79 - ETA: 0s - loss: 0.4414 - accuracy: 0.79 - ETA: 0s - loss: 0.4381 - accuracy: 0.79 - ETA: 0s - loss: 0.4375 - accuracy: 0.79 - 1s 85us/step - loss: 0.4389 - accuracy: 0.7929\n",
      "Epoch 23/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8400/8400 [==============================] - ETA: 0s - loss: 0.3815 - accuracy: 0.80 - ETA: 0s - loss: 0.4362 - accuracy: 0.78 - ETA: 0s - loss: 0.4376 - accuracy: 0.78 - ETA: 0s - loss: 0.4351 - accuracy: 0.79 - ETA: 0s - loss: 0.4344 - accuracy: 0.78 - ETA: 0s - loss: 0.4280 - accuracy: 0.79 - ETA: 0s - loss: 0.4290 - accuracy: 0.79 - ETA: 0s - loss: 0.4287 - accuracy: 0.79 - ETA: 0s - loss: 0.4321 - accuracy: 0.79 - ETA: 0s - loss: 0.4366 - accuracy: 0.79 - ETA: 0s - loss: 0.4363 - accuracy: 0.79 - ETA: 0s - loss: 0.4367 - accuracy: 0.79 - ETA: 0s - loss: 0.4336 - accuracy: 0.79 - ETA: 0s - loss: 0.4350 - accuracy: 0.79 - ETA: 0s - loss: 0.4369 - accuracy: 0.79 - 1s 86us/step - loss: 0.4366 - accuracy: 0.7979\n",
      "Epoch 24/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.5998 - accuracy: 0.70 - ETA: 0s - loss: 0.4521 - accuracy: 0.78 - ETA: 0s - loss: 0.4574 - accuracy: 0.78 - ETA: 0s - loss: 0.4310 - accuracy: 0.80 - ETA: 0s - loss: 0.4322 - accuracy: 0.80 - ETA: 0s - loss: 0.4304 - accuracy: 0.80 - ETA: 0s - loss: 0.4308 - accuracy: 0.80 - ETA: 0s - loss: 0.4276 - accuracy: 0.80 - ETA: 0s - loss: 0.4323 - accuracy: 0.80 - ETA: 0s - loss: 0.4358 - accuracy: 0.80 - ETA: 0s - loss: 0.4364 - accuracy: 0.80 - ETA: 0s - loss: 0.4354 - accuracy: 0.79 - ETA: 0s - loss: 0.4310 - accuracy: 0.80 - ETA: 0s - loss: 0.4318 - accuracy: 0.80 - 1s 84us/step - loss: 0.4348 - accuracy: 0.7981\n",
      "Epoch 25/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.3665 - accuracy: 0.80 - ETA: 0s - loss: 0.4103 - accuracy: 0.80 - ETA: 0s - loss: 0.4413 - accuracy: 0.79 - ETA: 0s - loss: 0.4359 - accuracy: 0.79 - ETA: 0s - loss: 0.4291 - accuracy: 0.80 - ETA: 0s - loss: 0.4292 - accuracy: 0.80 - ETA: 0s - loss: 0.4269 - accuracy: 0.80 - ETA: 0s - loss: 0.4319 - accuracy: 0.79 - ETA: 0s - loss: 0.4326 - accuracy: 0.79 - ETA: 0s - loss: 0.4344 - accuracy: 0.80 - ETA: 0s - loss: 0.4338 - accuracy: 0.80 - ETA: 0s - loss: 0.4354 - accuracy: 0.80 - ETA: 0s - loss: 0.4355 - accuracy: 0.80 - ETA: 0s - loss: 0.4338 - accuracy: 0.80 - ETA: 0s - loss: 0.4340 - accuracy: 0.80 - 1s 88us/step - loss: 0.4340 - accuracy: 0.8017\n",
      "Epoch 26/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.5564 - accuracy: 0.70 - ETA: 0s - loss: 0.4593 - accuracy: 0.78 - ETA: 0s - loss: 0.4502 - accuracy: 0.78 - ETA: 0s - loss: 0.4478 - accuracy: 0.78 - ETA: 0s - loss: 0.4435 - accuracy: 0.78 - ETA: 0s - loss: 0.4449 - accuracy: 0.78 - ETA: 0s - loss: 0.4424 - accuracy: 0.79 - ETA: 0s - loss: 0.4357 - accuracy: 0.79 - ETA: 0s - loss: 0.4337 - accuracy: 0.80 - ETA: 0s - loss: 0.4308 - accuracy: 0.80 - ETA: 0s - loss: 0.4285 - accuracy: 0.80 - ETA: 0s - loss: 0.4307 - accuracy: 0.80 - ETA: 0s - loss: 0.4288 - accuracy: 0.80 - ETA: 0s - loss: 0.4311 - accuracy: 0.80 - 1s 85us/step - loss: 0.4332 - accuracy: 0.8005\n",
      "Epoch 27/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.5108 - accuracy: 0.70 - ETA: 0s - loss: 0.4215 - accuracy: 0.80 - ETA: 0s - loss: 0.4293 - accuracy: 0.80 - ETA: 0s - loss: 0.4173 - accuracy: 0.81 - ETA: 0s - loss: 0.4151 - accuracy: 0.81 - ETA: 0s - loss: 0.4196 - accuracy: 0.81 - ETA: 0s - loss: 0.4268 - accuracy: 0.80 - ETA: 0s - loss: 0.4249 - accuracy: 0.80 - ETA: 0s - loss: 0.4260 - accuracy: 0.80 - ETA: 0s - loss: 0.4294 - accuracy: 0.80 - ETA: 0s - loss: 0.4302 - accuracy: 0.80 - ETA: 0s - loss: 0.4318 - accuracy: 0.80 - ETA: 0s - loss: 0.4337 - accuracy: 0.80 - ETA: 0s - loss: 0.4303 - accuracy: 0.80 - 1s 84us/step - loss: 0.4313 - accuracy: 0.8011\n",
      "Epoch 28/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2073 - accuracy: 1.00 - ETA: 0s - loss: 0.4279 - accuracy: 0.80 - ETA: 0s - loss: 0.4241 - accuracy: 0.80 - ETA: 0s - loss: 0.4177 - accuracy: 0.81 - ETA: 0s - loss: 0.4212 - accuracy: 0.81 - ETA: 0s - loss: 0.4261 - accuracy: 0.80 - ETA: 0s - loss: 0.4249 - accuracy: 0.80 - ETA: 0s - loss: 0.4291 - accuracy: 0.80 - ETA: 0s - loss: 0.4325 - accuracy: 0.80 - ETA: 0s - loss: 0.4309 - accuracy: 0.80 - ETA: 0s - loss: 0.4298 - accuracy: 0.80 - ETA: 0s - loss: 0.4299 - accuracy: 0.80 - ETA: 0s - loss: 0.4279 - accuracy: 0.80 - ETA: 0s - loss: 0.4297 - accuracy: 0.80 - 1s 84us/step - loss: 0.4299 - accuracy: 0.8062\n",
      "Epoch 29/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.5313 - accuracy: 0.60 - ETA: 0s - loss: 0.4373 - accuracy: 0.80 - ETA: 0s - loss: 0.4171 - accuracy: 0.81 - ETA: 0s - loss: 0.4040 - accuracy: 0.81 - ETA: 0s - loss: 0.4077 - accuracy: 0.81 - ETA: 0s - loss: 0.4189 - accuracy: 0.80 - ETA: 0s - loss: 0.4276 - accuracy: 0.80 - ETA: 0s - loss: 0.4279 - accuracy: 0.80 - ETA: 0s - loss: 0.4283 - accuracy: 0.80 - ETA: 0s - loss: 0.4285 - accuracy: 0.80 - ETA: 0s - loss: 0.4298 - accuracy: 0.80 - ETA: 0s - loss: 0.4278 - accuracy: 0.80 - ETA: 0s - loss: 0.4270 - accuracy: 0.80 - ETA: 0s - loss: 0.4261 - accuracy: 0.80 - 1s 83us/step - loss: 0.4291 - accuracy: 0.8061\n",
      "Epoch 30/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.1588 - accuracy: 1.00 - ETA: 0s - loss: 0.4532 - accuracy: 0.79 - ETA: 0s - loss: 0.4298 - accuracy: 0.81 - ETA: 0s - loss: 0.4324 - accuracy: 0.81 - ETA: 0s - loss: 0.4284 - accuracy: 0.81 - ETA: 0s - loss: 0.4283 - accuracy: 0.81 - ETA: 0s - loss: 0.4274 - accuracy: 0.81 - ETA: 0s - loss: 0.4251 - accuracy: 0.80 - ETA: 0s - loss: 0.4250 - accuracy: 0.80 - ETA: 0s - loss: 0.4299 - accuracy: 0.80 - ETA: 0s - loss: 0.4289 - accuracy: 0.80 - ETA: 0s - loss: 0.4282 - accuracy: 0.80 - ETA: 0s - loss: 0.4263 - accuracy: 0.80 - ETA: 0s - loss: 0.4256 - accuracy: 0.80 - 1s 84us/step - loss: 0.4276 - accuracy: 0.8071\n",
      "Epoch 31/75\n",
      "8400/8400 [==============================] - ETA: 20s - loss: 0.3900 - accuracy: 0.900 - ETA: 1s - loss: 0.4043 - accuracy: 0.820 - ETA: 0s - loss: 0.4244 - accuracy: 0.81 - ETA: 0s - loss: 0.4420 - accuracy: 0.80 - ETA: 0s - loss: 0.4347 - accuracy: 0.80 - ETA: 0s - loss: 0.4273 - accuracy: 0.80 - ETA: 0s - loss: 0.4332 - accuracy: 0.80 - ETA: 0s - loss: 0.4296 - accuracy: 0.80 - ETA: 0s - loss: 0.4266 - accuracy: 0.80 - ETA: 0s - loss: 0.4299 - accuracy: 0.80 - ETA: 0s - loss: 0.4315 - accuracy: 0.80 - ETA: 0s - loss: 0.4279 - accuracy: 0.80 - ETA: 0s - loss: 0.4322 - accuracy: 0.80 - ETA: 0s - loss: 0.4272 - accuracy: 0.80 - 1s 85us/step - loss: 0.4271 - accuracy: 0.8062\n",
      "Epoch 32/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2524 - accuracy: 0.90 - ETA: 0s - loss: 0.4090 - accuracy: 0.81 - ETA: 0s - loss: 0.4171 - accuracy: 0.81 - ETA: 0s - loss: 0.4236 - accuracy: 0.81 - ETA: 0s - loss: 0.4200 - accuracy: 0.81 - ETA: 0s - loss: 0.4209 - accuracy: 0.81 - ETA: 0s - loss: 0.4196 - accuracy: 0.81 - ETA: 0s - loss: 0.4222 - accuracy: 0.80 - ETA: 0s - loss: 0.4190 - accuracy: 0.81 - ETA: 0s - loss: 0.4178 - accuracy: 0.81 - ETA: 0s - loss: 0.4169 - accuracy: 0.81 - ETA: 0s - loss: 0.4196 - accuracy: 0.81 - ETA: 0s - loss: 0.4229 - accuracy: 0.81 - ETA: 0s - loss: 0.4242 - accuracy: 0.80 - 1s 83us/step - loss: 0.4250 - accuracy: 0.8068\n",
      "Epoch 33/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.7514 - accuracy: 0.60 - ETA: 0s - loss: 0.4158 - accuracy: 0.81 - ETA: 0s - loss: 0.4149 - accuracy: 0.81 - ETA: 0s - loss: 0.4238 - accuracy: 0.80 - ETA: 0s - loss: 0.4128 - accuracy: 0.81 - ETA: 0s - loss: 0.4128 - accuracy: 0.81 - ETA: 0s - loss: 0.4165 - accuracy: 0.81 - ETA: 0s - loss: 0.4151 - accuracy: 0.81 - ETA: 0s - loss: 0.4196 - accuracy: 0.81 - ETA: 0s - loss: 0.4226 - accuracy: 0.80 - ETA: 0s - loss: 0.4196 - accuracy: 0.81 - ETA: 0s - loss: 0.4230 - accuracy: 0.80 - ETA: 0s - loss: 0.4230 - accuracy: 0.80 - ETA: 0s - loss: 0.4242 - accuracy: 0.80 - ETA: 0s - loss: 0.4248 - accuracy: 0.80 - 1s 87us/step - loss: 0.4239 - accuracy: 0.8082\n",
      "Epoch 34/75\n",
      "8400/8400 [==============================] - ETA: 0s - loss: 0.3271 - accuracy: 0.90 - ETA: 0s - loss: 0.3745 - accuracy: 0.83 - ETA: 0s - loss: 0.3880 - accuracy: 0.82 - ETA: 0s - loss: 0.4052 - accuracy: 0.80 - ETA: 0s - loss: 0.4076 - accuracy: 0.81 - ETA: 0s - loss: 0.4121 - accuracy: 0.80 - ETA: 0s - loss: 0.4181 - accuracy: 0.80 - ETA: 0s - loss: 0.4186 - accuracy: 0.80 - ETA: 0s - loss: 0.4183 - accuracy: 0.80 - ETA: 0s - loss: 0.4214 - accuracy: 0.80 - ETA: 0s - loss: 0.4234 - accuracy: 0.80 - ETA: 0s - loss: 0.4241 - accuracy: 0.80 - ETA: 0s - loss: 0.4233 - accuracy: 0.80 - ETA: 0s - loss: 0.4246 - accuracy: 0.80 - ETA: 0s - loss: 0.4230 - accuracy: 0.80 - 1s 86us/step - loss: 0.4236 - accuracy: 0.8060\n",
      "Epoch 35/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.4373 - accuracy: 0.80 - ETA: 0s - loss: 0.4165 - accuracy: 0.81 - ETA: 0s - loss: 0.4165 - accuracy: 0.82 - ETA: 0s - loss: 0.4196 - accuracy: 0.81 - ETA: 0s - loss: 0.4099 - accuracy: 0.81 - ETA: 0s - loss: 0.4202 - accuracy: 0.81 - ETA: 0s - loss: 0.4229 - accuracy: 0.81 - ETA: 0s - loss: 0.4218 - accuracy: 0.80 - ETA: 0s - loss: 0.4219 - accuracy: 0.80 - ETA: 0s - loss: 0.4225 - accuracy: 0.80 - ETA: 0s - loss: 0.4222 - accuracy: 0.80 - ETA: 0s - loss: 0.4229 - accuracy: 0.80 - ETA: 0s - loss: 0.4223 - accuracy: 0.80 - ETA: 0s - loss: 0.4194 - accuracy: 0.81 - ETA: 0s - loss: 0.4225 - accuracy: 0.80 - 1s 86us/step - loss: 0.4225 - accuracy: 0.8085\n",
      "Epoch 36/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2594 - accuracy: 0.90 - ETA: 0s - loss: 0.4687 - accuracy: 0.77 - ETA: 0s - loss: 0.4297 - accuracy: 0.79 - ETA: 0s - loss: 0.4293 - accuracy: 0.79 - ETA: 0s - loss: 0.4268 - accuracy: 0.79 - ETA: 0s - loss: 0.4287 - accuracy: 0.80 - ETA: 0s - loss: 0.4287 - accuracy: 0.79 - ETA: 0s - loss: 0.4286 - accuracy: 0.80 - ETA: 0s - loss: 0.4246 - accuracy: 0.80 - ETA: 0s - loss: 0.4252 - accuracy: 0.80 - ETA: 0s - loss: 0.4255 - accuracy: 0.80 - ETA: 0s - loss: 0.4233 - accuracy: 0.80 - ETA: 0s - loss: 0.4249 - accuracy: 0.80 - ETA: 0s - loss: 0.4225 - accuracy: 0.80 - ETA: 0s - loss: 0.4211 - accuracy: 0.80 - 1s 86us/step - loss: 0.4216 - accuracy: 0.8094\n",
      "Epoch 37/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2604 - accuracy: 0.90 - ETA: 0s - loss: 0.4308 - accuracy: 0.80 - ETA: 0s - loss: 0.4174 - accuracy: 0.81 - ETA: 0s - loss: 0.4287 - accuracy: 0.80 - ETA: 0s - loss: 0.4264 - accuracy: 0.80 - ETA: 0s - loss: 0.4201 - accuracy: 0.81 - ETA: 0s - loss: 0.4230 - accuracy: 0.80 - ETA: 0s - loss: 0.4248 - accuracy: 0.80 - ETA: 0s - loss: 0.4247 - accuracy: 0.80 - ETA: 0s - loss: 0.4240 - accuracy: 0.80 - ETA: 0s - loss: 0.4237 - accuracy: 0.80 - ETA: 0s - loss: 0.4225 - accuracy: 0.81 - ETA: 0s - loss: 0.4197 - accuracy: 0.81 - ETA: 0s - loss: 0.4198 - accuracy: 0.81 - ETA: 0s - loss: 0.4203 - accuracy: 0.81 - 1s 90us/step - loss: 0.4207 - accuracy: 0.8106\n",
      "Epoch 38/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2907 - accuracy: 0.90 - ETA: 0s - loss: 0.4323 - accuracy: 0.79 - ETA: 0s - loss: 0.3986 - accuracy: 0.81 - ETA: 0s - loss: 0.4031 - accuracy: 0.81 - ETA: 0s - loss: 0.4084 - accuracy: 0.81 - ETA: 0s - loss: 0.4106 - accuracy: 0.81 - ETA: 0s - loss: 0.4139 - accuracy: 0.81 - ETA: 0s - loss: 0.4128 - accuracy: 0.81 - ETA: 0s - loss: 0.4177 - accuracy: 0.80 - ETA: 0s - loss: 0.4211 - accuracy: 0.80 - ETA: 0s - loss: 0.4225 - accuracy: 0.80 - ETA: 0s - loss: 0.4214 - accuracy: 0.80 - ETA: 0s - loss: 0.4197 - accuracy: 0.81 - ETA: 0s - loss: 0.4221 - accuracy: 0.80 - ETA: 0s - loss: 0.4196 - accuracy: 0.81 - 1s 86us/step - loss: 0.4195 - accuracy: 0.8105\n",
      "Epoch 39/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.6964 - accuracy: 0.50 - ETA: 0s - loss: 0.3994 - accuracy: 0.81 - ETA: 0s - loss: 0.4109 - accuracy: 0.80 - ETA: 0s - loss: 0.4037 - accuracy: 0.80 - ETA: 0s - loss: 0.4188 - accuracy: 0.80 - ETA: 0s - loss: 0.4102 - accuracy: 0.80 - ETA: 0s - loss: 0.4171 - accuracy: 0.80 - ETA: 0s - loss: 0.4187 - accuracy: 0.80 - ETA: 0s - loss: 0.4170 - accuracy: 0.80 - ETA: 0s - loss: 0.4167 - accuracy: 0.80 - ETA: 0s - loss: 0.4160 - accuracy: 0.80 - ETA: 0s - loss: 0.4144 - accuracy: 0.81 - ETA: 0s - loss: 0.4173 - accuracy: 0.80 - ETA: 0s - loss: 0.4168 - accuracy: 0.81 - 1s 82us/step - loss: 0.4179 - accuracy: 0.8098\n",
      "Epoch 40/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.4125 - accuracy: 0.90 - ETA: 0s - loss: 0.4165 - accuracy: 0.80 - ETA: 0s - loss: 0.4227 - accuracy: 0.80 - ETA: 0s - loss: 0.4248 - accuracy: 0.80 - ETA: 0s - loss: 0.4122 - accuracy: 0.81 - ETA: 0s - loss: 0.4173 - accuracy: 0.80 - ETA: 0s - loss: 0.4126 - accuracy: 0.81 - ETA: 0s - loss: 0.4092 - accuracy: 0.81 - ETA: 0s - loss: 0.4076 - accuracy: 0.81 - ETA: 0s - loss: 0.4081 - accuracy: 0.81 - ETA: 0s - loss: 0.4073 - accuracy: 0.81 - ETA: 0s - loss: 0.4123 - accuracy: 0.81 - ETA: 0s - loss: 0.4122 - accuracy: 0.81 - ETA: 0s - loss: 0.4156 - accuracy: 0.81 - ETA: 0s - loss: 0.4178 - accuracy: 0.81 - 1s 86us/step - loss: 0.4179 - accuracy: 0.8106\n",
      "Epoch 41/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.4054 - accuracy: 0.80 - ETA: 0s - loss: 0.3862 - accuracy: 0.83 - ETA: 0s - loss: 0.4002 - accuracy: 0.82 - ETA: 0s - loss: 0.4001 - accuracy: 0.82 - ETA: 0s - loss: 0.4000 - accuracy: 0.82 - ETA: 0s - loss: 0.4047 - accuracy: 0.82 - ETA: 0s - loss: 0.4012 - accuracy: 0.82 - ETA: 0s - loss: 0.4101 - accuracy: 0.81 - ETA: 0s - loss: 0.4092 - accuracy: 0.81 - ETA: 0s - loss: 0.4096 - accuracy: 0.81 - ETA: 0s - loss: 0.4178 - accuracy: 0.81 - ETA: 0s - loss: 0.4169 - accuracy: 0.81 - ETA: 0s - loss: 0.4166 - accuracy: 0.81 - ETA: 0s - loss: 0.4162 - accuracy: 0.81 - ETA: 0s - loss: 0.4167 - accuracy: 0.80 - 1s 89us/step - loss: 0.4168 - accuracy: 0.8093\n",
      "Epoch 42/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.4508 - accuracy: 0.90 - ETA: 0s - loss: 0.4437 - accuracy: 0.78 - ETA: 0s - loss: 0.4279 - accuracy: 0.80 - ETA: 0s - loss: 0.4205 - accuracy: 0.80 - ETA: 0s - loss: 0.4284 - accuracy: 0.80 - ETA: 0s - loss: 0.4290 - accuracy: 0.80 - ETA: 0s - loss: 0.4266 - accuracy: 0.80 - ETA: 0s - loss: 0.4204 - accuracy: 0.80 - ETA: 0s - loss: 0.4190 - accuracy: 0.80 - ETA: 0s - loss: 0.4174 - accuracy: 0.81 - ETA: 0s - loss: 0.4154 - accuracy: 0.81 - ETA: 0s - loss: 0.4123 - accuracy: 0.81 - ETA: 0s - loss: 0.4151 - accuracy: 0.81 - ETA: 0s - loss: 0.4163 - accuracy: 0.81 - ETA: 0s - loss: 0.4151 - accuracy: 0.81 - ETA: 0s - loss: 0.4154 - accuracy: 0.81 - 1s 92us/step - loss: 0.4150 - accuracy: 0.8112\n",
      "Epoch 43/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.7026 - accuracy: 0.80 - ETA: 0s - loss: 0.4315 - accuracy: 0.81 - ETA: 0s - loss: 0.4316 - accuracy: 0.80 - ETA: 0s - loss: 0.4270 - accuracy: 0.81 - ETA: 0s - loss: 0.4185 - accuracy: 0.81 - ETA: 0s - loss: 0.4210 - accuracy: 0.80 - ETA: 0s - loss: 0.4155 - accuracy: 0.81 - ETA: 0s - loss: 0.4169 - accuracy: 0.80 - ETA: 0s - loss: 0.4132 - accuracy: 0.81 - ETA: 0s - loss: 0.4110 - accuracy: 0.81 - ETA: 0s - loss: 0.4119 - accuracy: 0.81 - ETA: 0s - loss: 0.4117 - accuracy: 0.81 - ETA: 0s - loss: 0.4114 - accuracy: 0.81 - ETA: 0s - loss: 0.4136 - accuracy: 0.81 - ETA: 0s - loss: 0.4143 - accuracy: 0.81 - 1s 87us/step - loss: 0.4142 - accuracy: 0.8131\n",
      "Epoch 44/75\n",
      "8400/8400 [==============================] - ETA: 2s - loss: 0.3472 - accuracy: 0.80 - ETA: 0s - loss: 0.3968 - accuracy: 0.81 - ETA: 0s - loss: 0.4127 - accuracy: 0.80 - ETA: 0s - loss: 0.4039 - accuracy: 0.81 - ETA: 0s - loss: 0.4135 - accuracy: 0.80 - ETA: 0s - loss: 0.4088 - accuracy: 0.81 - ETA: 0s - loss: 0.4062 - accuracy: 0.81 - ETA: 0s - loss: 0.4134 - accuracy: 0.81 - ETA: 0s - loss: 0.4096 - accuracy: 0.81 - ETA: 0s - loss: 0.4056 - accuracy: 0.81 - ETA: 0s - loss: 0.4069 - accuracy: 0.81 - ETA: 0s - loss: 0.4075 - accuracy: 0.81 - ETA: 0s - loss: 0.4088 - accuracy: 0.81 - ETA: 0s - loss: 0.4121 - accuracy: 0.81 - ETA: 0s - loss: 0.4145 - accuracy: 0.81 - 1s 87us/step - loss: 0.4143 - accuracy: 0.8104\n",
      "Epoch 45/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8400/8400 [==============================] - ETA: 1s - loss: 0.1938 - accuracy: 1.00 - ETA: 0s - loss: 0.3965 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3984 - accuracy: 0.81 - ETA: 0s - loss: 0.4084 - accuracy: 0.81 - ETA: 0s - loss: 0.4065 - accuracy: 0.81 - ETA: 0s - loss: 0.4049 - accuracy: 0.81 - ETA: 0s - loss: 0.4090 - accuracy: 0.81 - ETA: 0s - loss: 0.4074 - accuracy: 0.81 - ETA: 0s - loss: 0.4080 - accuracy: 0.81 - ETA: 0s - loss: 0.4090 - accuracy: 0.81 - ETA: 0s - loss: 0.4104 - accuracy: 0.81 - ETA: 0s - loss: 0.4114 - accuracy: 0.81 - ETA: 0s - loss: 0.4113 - accuracy: 0.81 - 1s 83us/step - loss: 0.4128 - accuracy: 0.8119\n",
      "Epoch 46/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.1795 - accuracy: 1.00 - ETA: 0s - loss: 0.4492 - accuracy: 0.79 - ETA: 0s - loss: 0.4267 - accuracy: 0.81 - ETA: 0s - loss: 0.4071 - accuracy: 0.82 - ETA: 0s - loss: 0.4020 - accuracy: 0.82 - ETA: 0s - loss: 0.4065 - accuracy: 0.81 - ETA: 0s - loss: 0.4100 - accuracy: 0.81 - ETA: 0s - loss: 0.4133 - accuracy: 0.81 - ETA: 0s - loss: 0.4159 - accuracy: 0.81 - ETA: 0s - loss: 0.4168 - accuracy: 0.81 - ETA: 0s - loss: 0.4127 - accuracy: 0.81 - ETA: 0s - loss: 0.4110 - accuracy: 0.81 - ETA: 0s - loss: 0.4122 - accuracy: 0.81 - ETA: 0s - loss: 0.4124 - accuracy: 0.81 - 1s 85us/step - loss: 0.4123 - accuracy: 0.8146\n",
      "Epoch 47/75\n",
      "8400/8400 [==============================] - ETA: 0s - loss: 0.4427 - accuracy: 0.80 - ETA: 0s - loss: 0.3668 - accuracy: 0.84 - ETA: 0s - loss: 0.3884 - accuracy: 0.83 - ETA: 0s - loss: 0.3964 - accuracy: 0.82 - ETA: 0s - loss: 0.4009 - accuracy: 0.82 - ETA: 0s - loss: 0.4052 - accuracy: 0.82 - ETA: 0s - loss: 0.4078 - accuracy: 0.82 - ETA: 0s - loss: 0.4107 - accuracy: 0.81 - ETA: 0s - loss: 0.4099 - accuracy: 0.81 - ETA: 0s - loss: 0.4138 - accuracy: 0.81 - ETA: 0s - loss: 0.4153 - accuracy: 0.81 - ETA: 0s - loss: 0.4153 - accuracy: 0.81 - ETA: 0s - loss: 0.4159 - accuracy: 0.81 - ETA: 0s - loss: 0.4151 - accuracy: 0.81 - 1s 83us/step - loss: 0.4119 - accuracy: 0.8149\n",
      "Epoch 48/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2658 - accuracy: 0.90 - ETA: 0s - loss: 0.4124 - accuracy: 0.82 - ETA: 0s - loss: 0.4077 - accuracy: 0.81 - ETA: 0s - loss: 0.4001 - accuracy: 0.82 - ETA: 0s - loss: 0.4124 - accuracy: 0.81 - ETA: 0s - loss: 0.4106 - accuracy: 0.81 - ETA: 0s - loss: 0.4116 - accuracy: 0.81 - ETA: 0s - loss: 0.4172 - accuracy: 0.81 - ETA: 0s - loss: 0.4130 - accuracy: 0.81 - ETA: 0s - loss: 0.4123 - accuracy: 0.81 - ETA: 0s - loss: 0.4143 - accuracy: 0.81 - ETA: 0s - loss: 0.4130 - accuracy: 0.81 - ETA: 0s - loss: 0.4121 - accuracy: 0.81 - ETA: 0s - loss: 0.4141 - accuracy: 0.81 - ETA: 0s - loss: 0.4113 - accuracy: 0.81 - 1s 86us/step - loss: 0.4108 - accuracy: 0.8139\n",
      "Epoch 49/75\n",
      "8400/8400 [==============================] - ETA: 0s - loss: 0.5825 - accuracy: 0.80 - ETA: 0s - loss: 0.4214 - accuracy: 0.81 - ETA: 0s - loss: 0.4169 - accuracy: 0.81 - ETA: 0s - loss: 0.4178 - accuracy: 0.81 - ETA: 0s - loss: 0.4148 - accuracy: 0.81 - ETA: 0s - loss: 0.4140 - accuracy: 0.80 - ETA: 0s - loss: 0.4081 - accuracy: 0.81 - ETA: 0s - loss: 0.4086 - accuracy: 0.81 - ETA: 0s - loss: 0.4080 - accuracy: 0.81 - ETA: 0s - loss: 0.4121 - accuracy: 0.80 - ETA: 0s - loss: 0.4069 - accuracy: 0.81 - ETA: 0s - loss: 0.4058 - accuracy: 0.81 - ETA: 0s - loss: 0.4069 - accuracy: 0.81 - ETA: 0s - loss: 0.4063 - accuracy: 0.81 - 1s 84us/step - loss: 0.4102 - accuracy: 0.8146\n",
      "Epoch 50/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.6252 - accuracy: 0.80 - ETA: 0s - loss: 0.4016 - accuracy: 0.81 - ETA: 0s - loss: 0.4177 - accuracy: 0.80 - ETA: 0s - loss: 0.4018 - accuracy: 0.82 - ETA: 0s - loss: 0.3955 - accuracy: 0.82 - ETA: 0s - loss: 0.3958 - accuracy: 0.81 - ETA: 0s - loss: 0.4000 - accuracy: 0.81 - ETA: 0s - loss: 0.4009 - accuracy: 0.81 - ETA: 0s - loss: 0.4012 - accuracy: 0.81 - ETA: 0s - loss: 0.4022 - accuracy: 0.81 - ETA: 0s - loss: 0.4043 - accuracy: 0.81 - ETA: 0s - loss: 0.4033 - accuracy: 0.81 - ETA: 0s - loss: 0.4054 - accuracy: 0.81 - ETA: 0s - loss: 0.4063 - accuracy: 0.81 - 1s 85us/step - loss: 0.4092 - accuracy: 0.8149\n",
      "Epoch 51/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.3983 - accuracy: 0.90 - ETA: 0s - loss: 0.4119 - accuracy: 0.81 - ETA: 0s - loss: 0.4196 - accuracy: 0.81 - ETA: 0s - loss: 0.4101 - accuracy: 0.82 - ETA: 0s - loss: 0.4207 - accuracy: 0.81 - ETA: 0s - loss: 0.4223 - accuracy: 0.81 - ETA: 0s - loss: 0.4174 - accuracy: 0.81 - ETA: 0s - loss: 0.4115 - accuracy: 0.81 - ETA: 0s - loss: 0.4119 - accuracy: 0.81 - ETA: 0s - loss: 0.4141 - accuracy: 0.81 - ETA: 0s - loss: 0.4182 - accuracy: 0.81 - ETA: 0s - loss: 0.4168 - accuracy: 0.81 - ETA: 0s - loss: 0.4127 - accuracy: 0.81 - ETA: 0s - loss: 0.4103 - accuracy: 0.81 - 1s 84us/step - loss: 0.4084 - accuracy: 0.8182\n",
      "Epoch 52/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.3034 - accuracy: 0.90 - ETA: 0s - loss: 0.4223 - accuracy: 0.80 - ETA: 0s - loss: 0.3915 - accuracy: 0.82 - ETA: 0s - loss: 0.3874 - accuracy: 0.83 - ETA: 0s - loss: 0.3962 - accuracy: 0.82 - ETA: 0s - loss: 0.4010 - accuracy: 0.81 - ETA: 0s - loss: 0.4044 - accuracy: 0.81 - ETA: 0s - loss: 0.4014 - accuracy: 0.82 - ETA: 0s - loss: 0.4009 - accuracy: 0.82 - ETA: 0s - loss: 0.4031 - accuracy: 0.82 - ETA: 0s - loss: 0.4050 - accuracy: 0.82 - ETA: 0s - loss: 0.4048 - accuracy: 0.81 - ETA: 0s - loss: 0.4105 - accuracy: 0.81 - ETA: 0s - loss: 0.4105 - accuracy: 0.81 - 1s 84us/step - loss: 0.4088 - accuracy: 0.8173\n",
      "Epoch 53/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.5147 - accuracy: 0.70 - ETA: 0s - loss: 0.3662 - accuracy: 0.83 - ETA: 0s - loss: 0.3794 - accuracy: 0.83 - ETA: 0s - loss: 0.3815 - accuracy: 0.82 - ETA: 0s - loss: 0.3832 - accuracy: 0.82 - ETA: 0s - loss: 0.3859 - accuracy: 0.82 - ETA: 0s - loss: 0.3907 - accuracy: 0.82 - ETA: 0s - loss: 0.3953 - accuracy: 0.82 - ETA: 0s - loss: 0.3944 - accuracy: 0.82 - ETA: 0s - loss: 0.3979 - accuracy: 0.82 - ETA: 0s - loss: 0.3991 - accuracy: 0.82 - ETA: 0s - loss: 0.4013 - accuracy: 0.82 - ETA: 0s - loss: 0.4025 - accuracy: 0.82 - ETA: 0s - loss: 0.4039 - accuracy: 0.82 - ETA: 0s - loss: 0.4090 - accuracy: 0.81 - 1s 93us/step - loss: 0.4070 - accuracy: 0.8176\n",
      "Epoch 54/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.3094 - accuracy: 0.90 - ETA: 0s - loss: 0.3939 - accuracy: 0.82 - ETA: 0s - loss: 0.3905 - accuracy: 0.82 - ETA: 0s - loss: 0.3980 - accuracy: 0.82 - ETA: 0s - loss: 0.3934 - accuracy: 0.82 - ETA: 0s - loss: 0.3987 - accuracy: 0.82 - ETA: 0s - loss: 0.4009 - accuracy: 0.82 - ETA: 0s - loss: 0.4068 - accuracy: 0.81 - ETA: 0s - loss: 0.4038 - accuracy: 0.81 - ETA: 0s - loss: 0.4003 - accuracy: 0.82 - ETA: 0s - loss: 0.4017 - accuracy: 0.82 - ETA: 0s - loss: 0.4027 - accuracy: 0.82 - ETA: 0s - loss: 0.4042 - accuracy: 0.81 - ETA: 0s - loss: 0.4045 - accuracy: 0.82 - ETA: 0s - loss: 0.4075 - accuracy: 0.81 - 1s 90us/step - loss: 0.4069 - accuracy: 0.8193\n",
      "Epoch 55/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.3841 - accuracy: 0.80 - ETA: 0s - loss: 0.3889 - accuracy: 0.82 - ETA: 0s - loss: 0.3835 - accuracy: 0.82 - ETA: 0s - loss: 0.3932 - accuracy: 0.82 - ETA: 0s - loss: 0.4038 - accuracy: 0.81 - ETA: 0s - loss: 0.4067 - accuracy: 0.81 - ETA: 0s - loss: 0.4056 - accuracy: 0.81 - ETA: 0s - loss: 0.4094 - accuracy: 0.81 - ETA: 0s - loss: 0.4051 - accuracy: 0.81 - ETA: 0s - loss: 0.4026 - accuracy: 0.81 - ETA: 0s - loss: 0.4012 - accuracy: 0.82 - ETA: 0s - loss: 0.4027 - accuracy: 0.81 - ETA: 0s - loss: 0.4028 - accuracy: 0.82 - ETA: 0s - loss: 0.4038 - accuracy: 0.81 - ETA: 0s - loss: 0.4040 - accuracy: 0.81 - ETA: 0s - loss: 0.4052 - accuracy: 0.81 - 1s 93us/step - loss: 0.4061 - accuracy: 0.8180\n",
      "Epoch 56/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.1611 - accuracy: 1.00 - ETA: 0s - loss: 0.3654 - accuracy: 0.83 - ETA: 0s - loss: 0.3794 - accuracy: 0.83 - ETA: 0s - loss: 0.3748 - accuracy: 0.83 - ETA: 0s - loss: 0.3880 - accuracy: 0.82 - ETA: 0s - loss: 0.3966 - accuracy: 0.82 - ETA: 0s - loss: 0.3999 - accuracy: 0.82 - ETA: 0s - loss: 0.4063 - accuracy: 0.81 - ETA: 0s - loss: 0.4066 - accuracy: 0.81 - ETA: 0s - loss: 0.4073 - accuracy: 0.81 - ETA: 0s - loss: 0.4084 - accuracy: 0.81 - ETA: 0s - loss: 0.4061 - accuracy: 0.81 - ETA: 0s - loss: 0.4072 - accuracy: 0.81 - ETA: 0s - loss: 0.4063 - accuracy: 0.81 - ETA: 0s - loss: 0.4034 - accuracy: 0.81 - 1s 87us/step - loss: 0.4048 - accuracy: 0.8169\n",
      "Epoch 57/75\n",
      "8400/8400 [==============================] - ETA: 2s - loss: 0.4343 - accuracy: 0.60 - ETA: 0s - loss: 0.4248 - accuracy: 0.80 - ETA: 0s - loss: 0.3960 - accuracy: 0.82 - ETA: 0s - loss: 0.4005 - accuracy: 0.81 - ETA: 0s - loss: 0.4000 - accuracy: 0.81 - ETA: 0s - loss: 0.4051 - accuracy: 0.81 - ETA: 0s - loss: 0.4047 - accuracy: 0.81 - ETA: 0s - loss: 0.4082 - accuracy: 0.81 - ETA: 0s - loss: 0.4055 - accuracy: 0.81 - ETA: 0s - loss: 0.4048 - accuracy: 0.81 - ETA: 0s - loss: 0.4011 - accuracy: 0.82 - ETA: 0s - loss: 0.4029 - accuracy: 0.81 - ETA: 0s - loss: 0.4032 - accuracy: 0.82 - ETA: 0s - loss: 0.4046 - accuracy: 0.81 - ETA: 0s - loss: 0.4046 - accuracy: 0.81 - 1s 88us/step - loss: 0.4042 - accuracy: 0.8200\n",
      "Epoch 58/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2397 - accuracy: 1.00 - ETA: 0s - loss: 0.4071 - accuracy: 0.82 - ETA: 0s - loss: 0.3979 - accuracy: 0.82 - ETA: 0s - loss: 0.3949 - accuracy: 0.82 - ETA: 0s - loss: 0.3959 - accuracy: 0.81 - ETA: 0s - loss: 0.3977 - accuracy: 0.81 - ETA: 0s - loss: 0.3987 - accuracy: 0.81 - ETA: 0s - loss: 0.4000 - accuracy: 0.81 - ETA: 0s - loss: 0.3998 - accuracy: 0.81 - ETA: 0s - loss: 0.4028 - accuracy: 0.81 - ETA: 0s - loss: 0.4009 - accuracy: 0.81 - ETA: 0s - loss: 0.4018 - accuracy: 0.81 - ETA: 0s - loss: 0.4026 - accuracy: 0.81 - ETA: 0s - loss: 0.4025 - accuracy: 0.81 - ETA: 0s - loss: 0.4037 - accuracy: 0.81 - 1s 86us/step - loss: 0.4034 - accuracy: 0.8176\n",
      "Epoch 59/75\n",
      "8400/8400 [==============================] - ETA: 2s - loss: 0.8956 - accuracy: 0.70 - ETA: 0s - loss: 0.3967 - accuracy: 0.81 - ETA: 0s - loss: 0.4000 - accuracy: 0.82 - ETA: 0s - loss: 0.3845 - accuracy: 0.83 - ETA: 0s - loss: 0.3993 - accuracy: 0.82 - ETA: 0s - loss: 0.4008 - accuracy: 0.82 - ETA: 0s - loss: 0.3984 - accuracy: 0.82 - ETA: 0s - loss: 0.3978 - accuracy: 0.82 - ETA: 0s - loss: 0.3977 - accuracy: 0.82 - ETA: 0s - loss: 0.4010 - accuracy: 0.82 - ETA: 0s - loss: 0.4008 - accuracy: 0.82 - ETA: 0s - loss: 0.3999 - accuracy: 0.82 - ETA: 0s - loss: 0.4031 - accuracy: 0.82 - ETA: 0s - loss: 0.3998 - accuracy: 0.82 - ETA: 0s - loss: 0.4019 - accuracy: 0.81 - 1s 91us/step - loss: 0.4023 - accuracy: 0.8201\n",
      "Epoch 60/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.1960 - accuracy: 0.90 - ETA: 0s - loss: 0.3978 - accuracy: 0.81 - ETA: 0s - loss: 0.3791 - accuracy: 0.83 - ETA: 0s - loss: 0.3796 - accuracy: 0.83 - ETA: 0s - loss: 0.3885 - accuracy: 0.82 - ETA: 0s - loss: 0.3885 - accuracy: 0.82 - ETA: 0s - loss: 0.3868 - accuracy: 0.82 - ETA: 0s - loss: 0.3952 - accuracy: 0.82 - ETA: 0s - loss: 0.4001 - accuracy: 0.82 - ETA: 0s - loss: 0.4014 - accuracy: 0.81 - ETA: 0s - loss: 0.3957 - accuracy: 0.82 - ETA: 0s - loss: 0.3962 - accuracy: 0.82 - ETA: 0s - loss: 0.4007 - accuracy: 0.81 - ETA: 0s - loss: 0.4020 - accuracy: 0.81 - ETA: 0s - loss: 0.4030 - accuracy: 0.81 - 1s 86us/step - loss: 0.4016 - accuracy: 0.8194\n",
      "Epoch 61/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2000 - accuracy: 0.90 - ETA: 0s - loss: 0.3946 - accuracy: 0.83 - ETA: 0s - loss: 0.3905 - accuracy: 0.82 - ETA: 0s - loss: 0.3877 - accuracy: 0.83 - ETA: 0s - loss: 0.3907 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3949 - accuracy: 0.82 - ETA: 0s - loss: 0.3952 - accuracy: 0.81 - ETA: 0s - loss: 0.3916 - accuracy: 0.82 - ETA: 0s - loss: 0.3916 - accuracy: 0.82 - ETA: 0s - loss: 0.3966 - accuracy: 0.81 - ETA: 0s - loss: 0.4013 - accuracy: 0.81 - ETA: 0s - loss: 0.3992 - accuracy: 0.81 - ETA: 0s - loss: 0.4009 - accuracy: 0.81 - ETA: 0s - loss: 0.4025 - accuracy: 0.81 - 1s 87us/step - loss: 0.4018 - accuracy: 0.8186\n",
      "Epoch 62/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2396 - accuracy: 1.00 - ETA: 0s - loss: 0.3984 - accuracy: 0.83 - ETA: 0s - loss: 0.3957 - accuracy: 0.83 - ETA: 0s - loss: 0.3935 - accuracy: 0.82 - ETA: 0s - loss: 0.4021 - accuracy: 0.82 - ETA: 0s - loss: 0.4035 - accuracy: 0.81 - ETA: 0s - loss: 0.4045 - accuracy: 0.81 - ETA: 0s - loss: 0.4061 - accuracy: 0.81 - ETA: 0s - loss: 0.4119 - accuracy: 0.81 - ETA: 0s - loss: 0.4055 - accuracy: 0.81 - ETA: 0s - loss: 0.4077 - accuracy: 0.81 - ETA: 0s - loss: 0.4052 - accuracy: 0.81 - ETA: 0s - loss: 0.4039 - accuracy: 0.81 - ETA: 0s - loss: 0.4048 - accuracy: 0.81 - ETA: 0s - loss: 0.4027 - accuracy: 0.81 - 1s 89us/step - loss: 0.4008 - accuracy: 0.8190\n",
      "Epoch 63/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2816 - accuracy: 0.90 - ETA: 0s - loss: 0.3560 - accuracy: 0.85 - ETA: 0s - loss: 0.3920 - accuracy: 0.82 - ETA: 0s - loss: 0.3937 - accuracy: 0.82 - ETA: 0s - loss: 0.3927 - accuracy: 0.82 - ETA: 0s - loss: 0.3902 - accuracy: 0.83 - ETA: 0s - loss: 0.3936 - accuracy: 0.82 - ETA: 0s - loss: 0.3937 - accuracy: 0.82 - ETA: 0s - loss: 0.3933 - accuracy: 0.82 - ETA: 0s - loss: 0.3941 - accuracy: 0.82 - ETA: 0s - loss: 0.3936 - accuracy: 0.82 - ETA: 0s - loss: 0.3964 - accuracy: 0.82 - ETA: 0s - loss: 0.3994 - accuracy: 0.82 - ETA: 0s - loss: 0.4009 - accuracy: 0.82 - ETA: 0s - loss: 0.4013 - accuracy: 0.82 - 1s 86us/step - loss: 0.4009 - accuracy: 0.8211\n",
      "Epoch 64/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.5346 - accuracy: 0.70 - ETA: 0s - loss: 0.4160 - accuracy: 0.82 - ETA: 0s - loss: 0.4035 - accuracy: 0.82 - ETA: 0s - loss: 0.3984 - accuracy: 0.82 - ETA: 0s - loss: 0.4054 - accuracy: 0.82 - ETA: 0s - loss: 0.4012 - accuracy: 0.82 - ETA: 0s - loss: 0.4059 - accuracy: 0.82 - ETA: 0s - loss: 0.4068 - accuracy: 0.81 - ETA: 0s - loss: 0.4033 - accuracy: 0.81 - ETA: 0s - loss: 0.4004 - accuracy: 0.82 - ETA: 0s - loss: 0.3964 - accuracy: 0.82 - ETA: 0s - loss: 0.3966 - accuracy: 0.82 - ETA: 0s - loss: 0.3973 - accuracy: 0.82 - ETA: 0s - loss: 0.3962 - accuracy: 0.82 - ETA: 0s - loss: 0.3977 - accuracy: 0.82 - ETA: 0s - loss: 0.3999 - accuracy: 0.82 - 1s 92us/step - loss: 0.4003 - accuracy: 0.8205\n",
      "Epoch 65/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.1803 - accuracy: 0.90 - ETA: 0s - loss: 0.4073 - accuracy: 0.80 - ETA: 0s - loss: 0.4128 - accuracy: 0.80 - ETA: 0s - loss: 0.4238 - accuracy: 0.80 - ETA: 0s - loss: 0.4156 - accuracy: 0.81 - ETA: 0s - loss: 0.4120 - accuracy: 0.81 - ETA: 0s - loss: 0.4116 - accuracy: 0.81 - ETA: 0s - loss: 0.4155 - accuracy: 0.81 - ETA: 0s - loss: 0.4113 - accuracy: 0.81 - ETA: 0s - loss: 0.4098 - accuracy: 0.81 - ETA: 0s - loss: 0.4049 - accuracy: 0.81 - ETA: 0s - loss: 0.4023 - accuracy: 0.81 - ETA: 0s - loss: 0.3988 - accuracy: 0.82 - ETA: 0s - loss: 0.3989 - accuracy: 0.82 - ETA: 0s - loss: 0.3974 - accuracy: 0.82 - ETA: 0s - loss: 0.3987 - accuracy: 0.82 - 1s 93us/step - loss: 0.3991 - accuracy: 0.8229\n",
      "Epoch 66/75\n",
      "8400/8400 [==============================] - ETA: 2s - loss: 0.2697 - accuracy: 0.90 - ETA: 0s - loss: 0.3851 - accuracy: 0.82 - ETA: 0s - loss: 0.4110 - accuracy: 0.81 - ETA: 0s - loss: 0.4001 - accuracy: 0.82 - ETA: 0s - loss: 0.4074 - accuracy: 0.81 - ETA: 0s - loss: 0.4011 - accuracy: 0.82 - ETA: 0s - loss: 0.3968 - accuracy: 0.82 - ETA: 0s - loss: 0.3950 - accuracy: 0.82 - ETA: 0s - loss: 0.3937 - accuracy: 0.82 - ETA: 0s - loss: 0.3924 - accuracy: 0.82 - ETA: 0s - loss: 0.3928 - accuracy: 0.82 - ETA: 0s - loss: 0.3938 - accuracy: 0.82 - ETA: 0s - loss: 0.3962 - accuracy: 0.82 - ETA: 0s - loss: 0.3974 - accuracy: 0.82 - ETA: 0s - loss: 0.3992 - accuracy: 0.82 - ETA: 0s - loss: 0.3997 - accuracy: 0.82 - 1s 94us/step - loss: 0.3990 - accuracy: 0.8240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2358 - accuracy: 0.90 - ETA: 0s - loss: 0.3978 - accuracy: 0.83 - ETA: 0s - loss: 0.3865 - accuracy: 0.84 - ETA: 0s - loss: 0.3880 - accuracy: 0.83 - ETA: 0s - loss: 0.3909 - accuracy: 0.83 - ETA: 0s - loss: 0.3864 - accuracy: 0.83 - ETA: 0s - loss: 0.3877 - accuracy: 0.82 - ETA: 0s - loss: 0.3907 - accuracy: 0.82 - ETA: 0s - loss: 0.3956 - accuracy: 0.82 - ETA: 0s - loss: 0.4002 - accuracy: 0.82 - ETA: 0s - loss: 0.3975 - accuracy: 0.82 - ETA: 0s - loss: 0.3987 - accuracy: 0.82 - ETA: 0s - loss: 0.3951 - accuracy: 0.82 - ETA: 0s - loss: 0.3981 - accuracy: 0.82 - ETA: 0s - loss: 0.3982 - accuracy: 0.82 - 1s 86us/step - loss: 0.3986 - accuracy: 0.8219\n",
      "Epoch 68/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.5073 - accuracy: 0.50 - ETA: 0s - loss: 0.3564 - accuracy: 0.83 - ETA: 0s - loss: 0.3814 - accuracy: 0.81 - ETA: 0s - loss: 0.3881 - accuracy: 0.82 - ETA: 0s - loss: 0.3930 - accuracy: 0.81 - ETA: 0s - loss: 0.3969 - accuracy: 0.82 - ETA: 0s - loss: 0.3959 - accuracy: 0.82 - ETA: 0s - loss: 0.3908 - accuracy: 0.82 - ETA: 0s - loss: 0.3904 - accuracy: 0.82 - ETA: 0s - loss: 0.3954 - accuracy: 0.82 - ETA: 0s - loss: 0.3932 - accuracy: 0.82 - ETA: 0s - loss: 0.3963 - accuracy: 0.82 - ETA: 0s - loss: 0.3944 - accuracy: 0.82 - ETA: 0s - loss: 0.3972 - accuracy: 0.82 - ETA: 0s - loss: 0.3984 - accuracy: 0.82 - 1s 91us/step - loss: 0.3975 - accuracy: 0.8212\n",
      "Epoch 69/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2466 - accuracy: 0.90 - ETA: 0s - loss: 0.4145 - accuracy: 0.82 - ETA: 0s - loss: 0.3907 - accuracy: 0.83 - ETA: 0s - loss: 0.3962 - accuracy: 0.82 - ETA: 0s - loss: 0.3875 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3878 - accuracy: 0.82 - ETA: 0s - loss: 0.3874 - accuracy: 0.82 - ETA: 0s - loss: 0.3916 - accuracy: 0.82 - ETA: 0s - loss: 0.3927 - accuracy: 0.82 - ETA: 0s - loss: 0.3931 - accuracy: 0.82 - ETA: 0s - loss: 0.3935 - accuracy: 0.82 - ETA: 0s - loss: 0.3937 - accuracy: 0.82 - ETA: 0s - loss: 0.3929 - accuracy: 0.82 - ETA: 0s - loss: 0.3974 - accuracy: 0.82 - 1s 86us/step - loss: 0.3973 - accuracy: 0.8225\n",
      "Epoch 70/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.2484 - accuracy: 1.00 - ETA: 0s - loss: 0.4369 - accuracy: 0.81 - ETA: 0s - loss: 0.4106 - accuracy: 0.82 - ETA: 0s - loss: 0.3968 - accuracy: 0.82 - ETA: 0s - loss: 0.4002 - accuracy: 0.82 - ETA: 0s - loss: 0.3994 - accuracy: 0.82 - ETA: 0s - loss: 0.3976 - accuracy: 0.82 - ETA: 0s - loss: 0.3939 - accuracy: 0.82 - ETA: 0s - loss: 0.3873 - accuracy: 0.83 - ETA: 0s - loss: 0.3893 - accuracy: 0.83 - ETA: 0s - loss: 0.3896 - accuracy: 0.82 - ETA: 0s - loss: 0.3920 - accuracy: 0.82 - ETA: 0s - loss: 0.3961 - accuracy: 0.82 - ETA: 0s - loss: 0.3963 - accuracy: 0.82 - ETA: 0s - loss: 0.3971 - accuracy: 0.82 - 1s 89us/step - loss: 0.3967 - accuracy: 0.8236\n",
      "Epoch 71/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.4410 - accuracy: 0.80 - ETA: 0s - loss: 0.3757 - accuracy: 0.83 - ETA: 0s - loss: 0.3980 - accuracy: 0.81 - ETA: 0s - loss: 0.3934 - accuracy: 0.82 - ETA: 0s - loss: 0.3904 - accuracy: 0.82 - ETA: 0s - loss: 0.3904 - accuracy: 0.82 - ETA: 0s - loss: 0.3867 - accuracy: 0.82 - ETA: 0s - loss: 0.3938 - accuracy: 0.82 - ETA: 0s - loss: 0.3911 - accuracy: 0.82 - ETA: 0s - loss: 0.3921 - accuracy: 0.82 - ETA: 0s - loss: 0.3930 - accuracy: 0.82 - ETA: 0s - loss: 0.3933 - accuracy: 0.82 - ETA: 0s - loss: 0.3940 - accuracy: 0.82 - ETA: 0s - loss: 0.3944 - accuracy: 0.82 - 1s 82us/step - loss: 0.3961 - accuracy: 0.8232\n",
      "Epoch 72/75\n",
      "8400/8400 [==============================] - ETA: 0s - loss: 0.4501 - accuracy: 0.80 - ETA: 0s - loss: 0.4086 - accuracy: 0.83 - ETA: 0s - loss: 0.3940 - accuracy: 0.83 - ETA: 0s - loss: 0.4116 - accuracy: 0.82 - ETA: 0s - loss: 0.4078 - accuracy: 0.82 - ETA: 0s - loss: 0.4017 - accuracy: 0.82 - ETA: 0s - loss: 0.3996 - accuracy: 0.82 - ETA: 0s - loss: 0.3995 - accuracy: 0.82 - ETA: 0s - loss: 0.4007 - accuracy: 0.82 - ETA: 0s - loss: 0.3970 - accuracy: 0.82 - ETA: 0s - loss: 0.3958 - accuracy: 0.82 - ETA: 0s - loss: 0.3964 - accuracy: 0.82 - ETA: 0s - loss: 0.3966 - accuracy: 0.82 - ETA: 0s - loss: 0.3963 - accuracy: 0.82 - ETA: 0s - loss: 0.3973 - accuracy: 0.82 - 1s 88us/step - loss: 0.3962 - accuracy: 0.8233\n",
      "Epoch 73/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.7053 - accuracy: 0.60 - ETA: 0s - loss: 0.3713 - accuracy: 0.83 - ETA: 0s - loss: 0.3855 - accuracy: 0.83 - ETA: 0s - loss: 0.3970 - accuracy: 0.83 - ETA: 0s - loss: 0.3942 - accuracy: 0.83 - ETA: 0s - loss: 0.3945 - accuracy: 0.83 - ETA: 0s - loss: 0.3934 - accuracy: 0.83 - ETA: 0s - loss: 0.3938 - accuracy: 0.83 - ETA: 0s - loss: 0.3928 - accuracy: 0.83 - ETA: 0s - loss: 0.3957 - accuracy: 0.82 - ETA: 0s - loss: 0.3969 - accuracy: 0.82 - ETA: 0s - loss: 0.3961 - accuracy: 0.82 - ETA: 0s - loss: 0.3991 - accuracy: 0.82 - ETA: 0s - loss: 0.4009 - accuracy: 0.82 - ETA: 0s - loss: 0.3976 - accuracy: 0.82 - 1s 91us/step - loss: 0.3958 - accuracy: 0.8258\n",
      "Epoch 74/75\n",
      "8400/8400 [==============================] - ETA: 1s - loss: 0.7003 - accuracy: 0.70 - ETA: 0s - loss: 0.4288 - accuracy: 0.79 - ETA: 0s - loss: 0.4038 - accuracy: 0.80 - ETA: 0s - loss: 0.4009 - accuracy: 0.80 - ETA: 0s - loss: 0.3905 - accuracy: 0.82 - ETA: 0s - loss: 0.3850 - accuracy: 0.82 - ETA: 0s - loss: 0.3859 - accuracy: 0.82 - ETA: 0s - loss: 0.3858 - accuracy: 0.82 - ETA: 0s - loss: 0.3848 - accuracy: 0.82 - ETA: 0s - loss: 0.3877 - accuracy: 0.82 - ETA: 0s - loss: 0.3916 - accuracy: 0.82 - ETA: 0s - loss: 0.3940 - accuracy: 0.82 - ETA: 0s - loss: 0.3936 - accuracy: 0.82 - ETA: 0s - loss: 0.3943 - accuracy: 0.82 - ETA: 0s - loss: 0.3936 - accuracy: 0.82 - 1s 88us/step - loss: 0.3941 - accuracy: 0.8244\n",
      "Epoch 75/75\n",
      "8400/8400 [==============================] - ETA: 2s - loss: 0.2861 - accuracy: 0.90 - ETA: 0s - loss: 0.3918 - accuracy: 0.82 - ETA: 0s - loss: 0.3704 - accuracy: 0.83 - ETA: 0s - loss: 0.3758 - accuracy: 0.83 - ETA: 0s - loss: 0.3830 - accuracy: 0.83 - ETA: 0s - loss: 0.3770 - accuracy: 0.83 - ETA: 0s - loss: 0.3824 - accuracy: 0.83 - ETA: 0s - loss: 0.3841 - accuracy: 0.82 - ETA: 0s - loss: 0.3849 - accuracy: 0.83 - ETA: 0s - loss: 0.3829 - accuracy: 0.83 - ETA: 0s - loss: 0.3799 - accuracy: 0.83 - ETA: 0s - loss: 0.3803 - accuracy: 0.83 - ETA: 0s - loss: 0.3866 - accuracy: 0.83 - ETA: 0s - loss: 0.3880 - accuracy: 0.82 - ETA: 0s - loss: 0.3916 - accuracy: 0.82 - ETA: 0s - loss: 0.3939 - accuracy: 0.82 - 1s 93us/step - loss: 0.3940 - accuracy: 0.8258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x19c1a57b710>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "#First Hidden Layer\n",
    "classifier.add(Dense(params[0]['num'], activation=params[0]['act'], kernel_initializer='random_normal', input_dim=(len(features)-1)))\n",
    "#Second  Hidden Layer\n",
    "classifier.add(Dense(params[1]['num'], activation=params[1]['act'], kernel_initializer='random_normal'))\n",
    "#Output Layer\n",
    "classifier.add(Dense(1, activation=params[2]['act'], kernel_initializer='random_normal'))\n",
    "\n",
    "classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "\n",
    "classifier.fit(X, Y, batch_size=params[2]['batch'], epochs=params[2]['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1112  322]\n",
      " [ 382  704]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7206349206349206"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred2 = (y_pred>0.5)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred2)\n",
    "print(cm)\n",
    "acc = (cm[0][0] + cm[1][1])/sum(sum(cm))\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#fc0\">Запись результатов</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras\n",
    "test = sc.fit_transform(my_data_test.fillna(0))\n",
    "y_pred = classifier.predict(test)\n",
    "pd.DataFrame(data={'probability':list(y_pred)}, index=my_data_test.index).to_csv('../data/submission_keras.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
